{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTS.api import TTS\n",
    "\n",
    "tts = TTS(model_name=\"tts_models/en/ljspeech/tacotron2-DDC\", progress_bar=False, gpu=False)\n",
    "tts.tts_to_file(text=\"Hola, cómo estás?\", file_path=\"output.wav\",lang='es' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required module for text \n",
    "# to speech conversion\n",
    "from gtts import gTTS\n",
    "\n",
    "# This module is imported so that we can \n",
    "# play the converted audio\n",
    "import os\n",
    "\n",
    "#import playsound module to reproduce audio\n",
    "#from playsound import playsound\n",
    "import pygame\n",
    "\n",
    "# The text that you want to convert to audio\n",
    "mytext = 'Vamos a probar este audio, si sale cada palabra en orden estamos bien!'\n",
    "\n",
    "# Language in which you want to convert\n",
    "language = 'es'\n",
    "\n",
    "# Passing the text and language to the engine, \n",
    "# here we have marked slow=False. Which tells \n",
    "# the module that the converted audio should \n",
    "# have a high speed\n",
    "myobj = gTTS(text=mytext, lang=language, slow=False)\n",
    "\n",
    "# Saving the converted audio in a mp3 file named\n",
    "# welcome \n",
    "myobj.save(\"output1.mp3\")\n",
    "\n",
    "# Playing the converted file\n",
    "#os.system(\"start welcome.wav\")\n",
    "#music=r'esp.wav'\n",
    "# Initialize the mixer module\n",
    "pygame.mixer.init()\n",
    "pygame.mixer.music.load(\"output1.mp3\")\n",
    "# Load the mp3 file\n",
    "#playsound(\"welcome.wav\")\n",
    "\n",
    "# Play the loaded mp3 file\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/en/ljspeech/tacotron2-DDC is already downloaded.\n",
      " > vocoder_models/en/ljspeech/hifigan_v2 is already downloaded.\n",
      " > Using model: Tacotron2\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:2.718281828459045\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Model's reduction rate `r` is set to: 1\n",
      " > Vocoder Model: hifigan\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:2.718281828459045\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Generator Model: hifigan_generator\n",
      " > Discriminator Model: hifigan_discriminator\n",
      "Removing weight norm...\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "from TTS.api import TTS\n",
    "import subprocess\n",
    "from textblob import TextBlob\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "# Initialize TTS\n",
    "tts = TTS(model_name=\"tts_models/en/ljspeech/tacotron2-DDC\", progress_bar=False, gpu=False)\n",
    "\n",
    "# Function to detect emotion\n",
    "def detect_emotion(text):\n",
    "    blob = TextBlob(text)\n",
    "    sentiment = blob.sentiment.polarity\n",
    "    if sentiment > 0.2:\n",
    "        return \"happy\"\n",
    "    elif sentiment < -0.2:\n",
    "        return \"sad\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "# Function to animate avatar (placeholder)\n",
    "def animate_avatar(emotion, lip_sync_data):\n",
    "    print(f\"Displaying {emotion} expression with lip-sync data: {lip_sync_data}\")\n",
    "    # Load the appropriate avatar image based on emotion\n",
    "    avatar = Image.open(f\"avatar_{emotion}.png\")\n",
    "    avatar.show()\n",
    "\n",
    "# Main loop\n",
    "while True:\n",
    "    # Get user input\n",
    "    user_input = input(\"You: \")\n",
    "\n",
    "    # Generate response using Ollama\n",
    "    response = ollama.generate(model=\"llama2\", prompt=user_input)[\"response\"]\n",
    "    print(f\"Avatar: {response}\")\n",
    "\n",
    "    # Detect emotion\n",
    "    emotion = detect_emotion(response)\n",
    "\n",
    "    # Convert response to speech\n",
    "    tts.tts_to_file(text=response, file_path=\"output.wav\")\n",
    "\n",
    "    # Generate lip-sync data using Rhubarb Lip Sync\n",
    "    subprocess.run([\"rhubarb\", \"-o\", \"lip_sync.json\", \"output.wav\"])\n",
    "\n",
    "    # Load lip-sync data (simplified)\n",
    "    with open(\"lip_sync.json\", \"r\") as f:\n",
    "        lip_sync_data = f.read()\n",
    "\n",
    "    # Animate avatar\n",
    "    animate_avatar(emotion, lip_sync_data)\n",
    "\n",
    "    # Wait for next input\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.generate(model=\"llama3.2:latest\", prompt=user_input, options={\"num_predict\": 20})[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ChatOllama in module langchain_ollama.chat_models:\n",
      "\n",
      "class ChatOllama(langchain_core.language_models.chat_models.BaseChatModel)\n",
      " |  ChatOllama(*args: Any, name: Optional[str] = None, cache: Union[langchain_core.caches.BaseCache, bool, NoneType] = None, verbose: bool = <factory>, callbacks: Union[list[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, tags: Optional[list[str]] = None, metadata: Optional[dict[str, Any]] = None, custom_get_token_ids: Optional[Callable[[str], list[int]]] = None, callback_manager: Optional[langchain_core.callbacks.base.BaseCallbackManager] = None, rate_limiter: Optional[langchain_core.rate_limiters.BaseRateLimiter] = None, disable_streaming: Union[bool, Literal['tool_calling']] = False, model: str, mirostat: Optional[int] = None, mirostat_eta: Optional[float] = None, mirostat_tau: Optional[float] = None, num_ctx: Optional[int] = None, num_gpu: Optional[int] = None, num_thread: Optional[int] = None, num_predict: Optional[int] = None, repeat_last_n: Optional[int] = None, repeat_penalty: Optional[float] = None, temperature: Optional[float] = None, seed: Optional[int] = None, stop: Optional[List[str]] = None, tfs_z: Optional[float] = None, top_k: Optional[int] = None, top_p: Optional[float] = None, format: Union[Literal['', 'json'], Dict[str, Any], NoneType] = None, keep_alive: Union[int, str, NoneType] = None, base_url: Optional[str] = None, client_kwargs: Optional[dict] = {}) -> None\n",
      " |  \n",
      " |  Ollama chat model integration.\n",
      " |  \n",
      " |  .. dropdown:: Setup\n",
      " |      :open:\n",
      " |  \n",
      " |      Install ``langchain-ollama`` and download any models you want to use from ollama.\n",
      " |  \n",
      " |      .. code-block:: bash\n",
      " |  \n",
      " |          ollama pull mistral:v0.3\n",
      " |          pip install -U langchain-ollama\n",
      " |  \n",
      " |  Key init args — completion params:\n",
      " |      model: str\n",
      " |          Name of Ollama model to use.\n",
      " |      temperature: float\n",
      " |          Sampling temperature. Ranges from 0.0 to 1.0.\n",
      " |      num_predict: Optional[int]\n",
      " |          Max number of tokens to generate.\n",
      " |  \n",
      " |  See full list of supported init args and their descriptions in the params section.\n",
      " |  \n",
      " |  Instantiate:\n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |          from langchain_ollama import ChatOllama\n",
      " |  \n",
      " |          llm = ChatOllama(\n",
      " |              model = \"llama3\",\n",
      " |              temperature = 0.8,\n",
      " |              num_predict = 256,\n",
      " |              # other params ...\n",
      " |          )\n",
      " |  \n",
      " |  Invoke:\n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |          messages = [\n",
      " |              (\"system\", \"You are a helpful translator. Translate the user sentence to French.\"),\n",
      " |              (\"human\", \"I love programming.\"),\n",
      " |          ]\n",
      " |          llm.invoke(messages)\n",
      " |  \n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |          AIMessage(content='J'adore le programmation. (Note: \"programming\" can also refer to the act of writing code, so if you meant that, I could translate it as \"J'adore programmer\". But since you didn\\'t specify, I assumed you were talking about the activity itself, which is what \"le programmation\" usually refers to.)', response_metadata={'model': 'llama3', 'created_at': '2024-07-04T03:37:50.182604Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 3576619666, 'load_duration': 788524916, 'prompt_eval_count': 32, 'prompt_eval_duration': 128125000, 'eval_count': 71, 'eval_duration': 2656556000}, id='run-ba48f958-6402-41a5-b461-5e250a4ebd36-0')\n",
      " |  \n",
      " |  Stream:\n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |          messages = [\n",
      " |              (\"human\", \"Return the words Hello World!\"),\n",
      " |          ]\n",
      " |          for chunk in llm.stream(messages):\n",
      " |              print(chunk)\n",
      " |  \n",
      " |  \n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |          content='Hello' id='run-327ff5ad-45c8-49fe-965c-0a93982e9be1'\n",
      " |          content=' World' id='run-327ff5ad-45c8-49fe-965c-0a93982e9be1'\n",
      " |          content='!' id='run-327ff5ad-45c8-49fe-965c-0a93982e9be1'\n",
      " |          content='' response_metadata={'model': 'llama3', 'created_at': '2024-07-04T03:39:42.274449Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 411875125, 'load_duration': 1898166, 'prompt_eval_count': 14, 'prompt_eval_duration': 297320000, 'eval_count': 4, 'eval_duration': 111099000} id='run-327ff5ad-45c8-49fe-965c-0a93982e9be1'\n",
      " |  \n",
      " |  \n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |          stream = llm.stream(messages)\n",
      " |          full = next(stream)\n",
      " |          for chunk in stream:\n",
      " |              full += chunk\n",
      " |          full\n",
      " |  \n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |          AIMessageChunk(content='Je adore le programmation.(Note: \"programmation\" is the formal way to say \"programming\" in French, but informally, people might use the phrase \"le développement logiciel\" or simply \"le code\")', response_metadata={'model': 'llama3', 'created_at': '2024-07-04T03:38:54.933154Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 1977300042, 'load_duration': 1345709, 'prompt_eval_duration': 159343000, 'eval_count': 47, 'eval_duration': 1815123000}, id='run-3c81a3ed-3e79-4dd3-a796-04064d804890')\n",
      " |  \n",
      " |  Async:\n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |          messages = [\n",
      " |              (\"human\", \"Hello how are you!\"),\n",
      " |          ]\n",
      " |          await llm.ainvoke(messages)\n",
      " |  \n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |          AIMessage(content=\"Hi there! I'm just an AI, so I don't have feelings or emotions like humans do. But I'm functioning properly and ready to help with any questions or tasks you may have! How can I assist you today?\", response_metadata={'model': 'llama3', 'created_at': '2024-07-04T03:52:08.165478Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 2138492875, 'load_duration': 1364000, 'prompt_eval_count': 10, 'prompt_eval_duration': 297081000, 'eval_count': 47, 'eval_duration': 1838524000}, id='run-29c510ae-49a4-4cdd-8f23-b972bfab1c49-0')\n",
      " |  \n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |          messages = [\n",
      " |              (\"human\", \"Say hello world!\"),\n",
      " |          ]\n",
      " |          async for chunk in llm.astream(messages):\n",
      " |              print(chunk.content)\n",
      " |  \n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |          HEL\n",
      " |          LO\n",
      " |          WORLD\n",
      " |          !\n",
      " |  \n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |          messages = [\n",
      " |              (\"human\", \"Say hello world!\"),\n",
      " |              (\"human\",\"Say goodbye world!\")\n",
      " |          ]\n",
      " |          await llm.abatch(messages)\n",
      " |  \n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |          [AIMessage(content='HELLO, WORLD!', response_metadata={'model': 'llama3', 'created_at': '2024-07-04T03:55:07.315396Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 1696745458, 'load_duration': 1505000, 'prompt_eval_count': 8, 'prompt_eval_duration': 111627000, 'eval_count': 6, 'eval_duration': 185181000}, id='run-da6c7562-e25a-4a44-987a-2c83cd8c2686-0'),\n",
      " |          AIMessage(content=\"It's been a blast chatting with you! Say goodbye to the world for me, and don't forget to come back and visit us again soon!\", response_metadata={'model': 'llama3', 'created_at': '2024-07-04T03:55:07.018076Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 1399391083, 'load_duration': 1187417, 'prompt_eval_count': 20, 'prompt_eval_duration': 230349000, 'eval_count': 31, 'eval_duration': 1166047000}, id='run-96cad530-6f3e-4cf9-86b4-e0f8abba4cdb-0')]\n",
      " |  \n",
      " |  JSON mode:\n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |  \n",
      " |          json_llm = ChatOllama(format=\"json\")\n",
      " |          messages = [\n",
      " |              (\"human\", \"Return a query for the weather in a random location and time of day with two keys: location and time_of_day. Respond using JSON only.\"),\n",
      " |          ]\n",
      " |          llm.invoke(messages).content\n",
      " |  \n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |          '{\"location\": \"Pune, India\", \"time_of_day\": \"morning\"}'\n",
      " |  \n",
      " |  Tool Calling:\n",
      " |  \n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |          from langchain_ollama import ChatOllama\n",
      " |          from pydantic import BaseModel, Field\n",
      " |  \n",
      " |          class Multiply(BaseModel):\n",
      " |              a: int = Field(..., description=\"First integer\")\n",
      " |              b: int = Field(..., description=\"Second integer\")\n",
      " |  \n",
      " |          ans = await chat.invoke(\"What is 45*67\")\n",
      " |          ans.tool_calls\n",
      " |  \n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |          [{'name': 'Multiply',\n",
      " |          'args': {'a': 45, 'b': 67},\n",
      " |          'id': '420c3f3b-df10-4188-945f-eb3abdb40622',\n",
      " |          'type': 'tool_call'}]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ChatOllama\n",
      " |      langchain_core.language_models.chat_models.BaseChatModel\n",
      " |      langchain_core.language_models.base.BaseLanguageModel[BaseMessage]\n",
      " |      langchain_core.language_models.base.BaseLanguageModel\n",
      " |      langchain_core.runnables.base.RunnableSerializable[Union[PromptValue, str, Sequence[Union[BaseMessage, list[str], tuple[str, str], str, dict[str, Any]]]], ~LanguageModelOutputVar]\n",
      " |      langchain_core.runnables.base.RunnableSerializable\n",
      " |      langchain_core.load.serializable.Serializable\n",
      " |      pydantic.main.BaseModel\n",
      " |      langchain_core.runnables.base.Runnable\n",
      " |      typing.Generic\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  bind_tools(self, tools: Sequence[Union[Dict[str, Any], Type, Callable, langchain_core.tools.base.BaseTool]], *, tool_choice: Union[dict, str, Literal['auto', 'any'], bool, NoneType] = None, **kwargs: Any) -> langchain_core.runnables.base.Runnable[typing.Union[langchain_core.prompt_values.PromptValue, str, collections.abc.Sequence[typing.Union[langchain_core.messages.base.BaseMessage, list[str], tuple[str, str], str, dict[str, typing.Any]]]], langchain_core.messages.base.BaseMessage]\n",
      " |      Bind tool-like objects to this chat model.\n",
      " |      \n",
      " |      Assumes model is compatible with OpenAI tool-calling API.\n",
      " |      \n",
      " |      Args:\n",
      " |          tools: A list of tool definitions to bind to this chat model.\n",
      " |              Supports any tool definition handled by\n",
      " |              :meth:`langchain_core.utils.function_calling.convert_to_openai_tool`.\n",
      " |          tool_choice: If provided, which tool for model to call. **This parameter\n",
      " |              is currently ignored as it is not supported by Ollama.**\n",
      " |          kwargs: Any additional parameters are passed directly to\n",
      " |              ``self.bind(**kwargs)``.\n",
      " |  \n",
      " |  model_post_init = init_private_attributes(self: 'BaseModel', context: 'Any', /) -> 'None' from pydantic._internal._model_construction\n",
      " |      This function is meant to behave like a BaseModel method to initialise private attributes.\n",
      " |      \n",
      " |      It takes context as an argument since that's what pydantic-core passes when calling it.\n",
      " |      \n",
      " |      Args:\n",
      " |          self: The BaseModel instance.\n",
      " |          context: The context.\n",
      " |  \n",
      " |  with_structured_output(self, schema: Union[Dict, type], *, method: Literal['function_calling', 'json_mode', 'json_schema'] = 'function_calling', include_raw: bool = False, **kwargs: Any) -> langchain_core.runnables.base.Runnable[typing.Union[langchain_core.prompt_values.PromptValue, str, collections.abc.Sequence[typing.Union[langchain_core.messages.base.BaseMessage, list[str], tuple[str, str], str, dict[str, typing.Any]]]], typing.Union[typing.Dict, pydantic.main.BaseModel]]\n",
      " |      Model wrapper that returns outputs formatted to match the given schema.\n",
      " |      \n",
      " |      Args:\n",
      " |          schema:\n",
      " |              The output schema. Can be passed in as:\n",
      " |      \n",
      " |              - a Pydantic class,\n",
      " |              - a JSON schema\n",
      " |              - a TypedDict class\n",
      " |              - an OpenAI function/tool schema.\n",
      " |      \n",
      " |              If ``schema`` is a Pydantic class then the model output will be a\n",
      " |              Pydantic instance of that class, and the model-generated fields will be\n",
      " |              validated by the Pydantic class. Otherwise the model output will be a\n",
      " |              dict and will not be validated. See :meth:`langchain_core.utils.function_calling.convert_to_openai_tool`\n",
      " |              for more on how to properly specify types and descriptions of\n",
      " |              schema fields when specifying a Pydantic or TypedDict class.\n",
      " |      \n",
      " |          method: The method for steering model generation, one of:\n",
      " |      \n",
      " |              - \"function_calling\":\n",
      " |                  Uses Ollama's tool-calling API\n",
      " |              - \"json_schema\":\n",
      " |                  Uses Ollama's structured output API: https://ollama.com/blog/structured-outputs\n",
      " |              - \"json_mode\":\n",
      " |                  Specifies ``format=\"json\"``. Note that if using JSON mode then you\n",
      " |                  must include instructions for formatting the output into the\n",
      " |                  desired schema into the model call.\n",
      " |      \n",
      " |          include_raw:\n",
      " |              If False then only the parsed structured output is returned. If\n",
      " |              an error occurs during model output parsing it will be raised. If True\n",
      " |              then both the raw model response (a BaseMessage) and the parsed model\n",
      " |              response will be returned. If an error occurs during output parsing it\n",
      " |              will be caught and returned as well. The final output is always a dict\n",
      " |              with keys \"raw\", \"parsed\", and \"parsing_error\".\n",
      " |      \n",
      " |          kwargs: Additional keyword args aren't supported.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A Runnable that takes same inputs as a :class:`langchain_core.language_models.chat.BaseChatModel`.\n",
      " |      \n",
      " |          | If ``include_raw`` is False and ``schema`` is a Pydantic class, Runnable outputs an instance of ``schema`` (i.e., a Pydantic object). Otherwise, if ``include_raw`` is False then Runnable outputs a dict.\n",
      " |      \n",
      " |          | If ``include_raw`` is True, then Runnable outputs a dict with keys:\n",
      " |      \n",
      " |          - \"raw\": BaseMessage\n",
      " |          - \"parsed\": None if there was a parsing error, otherwise the type depends on the ``schema`` as described above.\n",
      " |          - \"parsing_error\": Optional[BaseException]\n",
      " |      \n",
      " |      .. versionchanged:: 0.2.2\n",
      " |      \n",
      " |          Added support for structured output API via ``format`` parameter.\n",
      " |      \n",
      " |      .. dropdown:: Example: schema=Pydantic class, method=\"function_calling\", include_raw=False\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              from typing import Optional\n",
      " |      \n",
      " |              from langchain_ollama import ChatOllama\n",
      " |              from pydantic import BaseModel, Field\n",
      " |      \n",
      " |      \n",
      " |              class AnswerWithJustification(BaseModel):\n",
      " |                  '''An answer to the user question along with justification for the answer.'''\n",
      " |      \n",
      " |                  answer: str\n",
      " |                  justification: Optional[str] = Field(\n",
      " |                      default=..., description=\"A justification for the answer.\"\n",
      " |                  )\n",
      " |      \n",
      " |      \n",
      " |              llm = ChatOllama(model=\"llama3.1\", temperature=0)\n",
      " |              structured_llm = llm.with_structured_output(\n",
      " |                  AnswerWithJustification\n",
      " |              )\n",
      " |      \n",
      " |              structured_llm.invoke(\n",
      " |                  \"What weighs more a pound of bricks or a pound of feathers\"\n",
      " |              )\n",
      " |      \n",
      " |              # -> AnswerWithJustification(\n",
      " |              #     answer='They weigh the same',\n",
      " |              #     justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'\n",
      " |              # )\n",
      " |      \n",
      " |      .. dropdown:: Example: schema=Pydantic class, method=\"function_calling\", include_raw=True\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              from langchain_ollama import ChatOllama\n",
      " |              from pydantic import BaseModel\n",
      " |      \n",
      " |      \n",
      " |              class AnswerWithJustification(BaseModel):\n",
      " |                  '''An answer to the user question along with justification for the answer.'''\n",
      " |      \n",
      " |                  answer: str\n",
      " |                  justification: str\n",
      " |      \n",
      " |      \n",
      " |              llm = ChatOllama(model=\"llama3.1\", temperature=0)\n",
      " |              structured_llm = llm.with_structured_output(\n",
      " |                  AnswerWithJustification, include_raw=True\n",
      " |              )\n",
      " |      \n",
      " |              structured_llm.invoke(\n",
      " |                  \"What weighs more a pound of bricks or a pound of feathers\"\n",
      " |              )\n",
      " |              # -> {\n",
      " |              #     'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Ao02pnFYXD6GN1yzc0uXPsvF', 'function': {'arguments': '{\"answer\":\"They weigh the same.\",\"justification\":\"Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.\"}', 'name': 'AnswerWithJustification'}, 'type': 'function'}]}),\n",
      " |              #     'parsed': AnswerWithJustification(answer='They weigh the same.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'),\n",
      " |              #     'parsing_error': None\n",
      " |              # }\n",
      " |      \n",
      " |      .. dropdown:: Example: schema=Pydantic class, method=\"json_schema\", include_raw=False\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              from typing import Optional\n",
      " |      \n",
      " |              from langchain_ollama import ChatOllama\n",
      " |              from pydantic import BaseModel, Field\n",
      " |      \n",
      " |      \n",
      " |              class AnswerWithJustification(BaseModel):\n",
      " |                  '''An answer to the user question along with justification for the answer.'''\n",
      " |      \n",
      " |                  answer: str\n",
      " |                  justification: Optional[str] = Field(\n",
      " |                      default=..., description=\"A justification for the answer.\"\n",
      " |                  )\n",
      " |      \n",
      " |      \n",
      " |              llm = ChatOllama(model=\"llama3.1\", temperature=0)\n",
      " |              structured_llm = llm.with_structured_output(\n",
      " |                  AnswerWithJustification, method=\"json_schema\"\n",
      " |              )\n",
      " |      \n",
      " |              structured_llm.invoke(\n",
      " |                  \"What weighs more a pound of bricks or a pound of feathers\"\n",
      " |              )\n",
      " |      \n",
      " |              # -> AnswerWithJustification(\n",
      " |              #     answer='They weigh the same',\n",
      " |              #     justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'\n",
      " |              # )\n",
      " |      \n",
      " |      .. dropdown:: Example: schema=TypedDict class, method=\"function_calling\", include_raw=False\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              # IMPORTANT: If you are using Python <=3.8, you need to import Annotated\n",
      " |              # from typing_extensions, not from typing.\n",
      " |              from typing_extensions import Annotated, TypedDict\n",
      " |      \n",
      " |              from langchain_ollama import ChatOllama\n",
      " |      \n",
      " |      \n",
      " |              class AnswerWithJustification(TypedDict):\n",
      " |                  '''An answer to the user question along with justification for the answer.'''\n",
      " |      \n",
      " |                  answer: str\n",
      " |                  justification: Annotated[\n",
      " |                      Optional[str], None, \"A justification for the answer.\"\n",
      " |                  ]\n",
      " |      \n",
      " |      \n",
      " |              llm = ChatOllama(model=\"llama3.1\", temperature=0)\n",
      " |              structured_llm = llm.with_structured_output(AnswerWithJustification)\n",
      " |      \n",
      " |              structured_llm.invoke(\n",
      " |                  \"What weighs more a pound of bricks or a pound of feathers\"\n",
      " |              )\n",
      " |              # -> {\n",
      " |              #     'answer': 'They weigh the same',\n",
      " |              #     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'\n",
      " |              # }\n",
      " |      \n",
      " |      .. dropdown:: Example: schema=OpenAI function schema, method=\"function_calling\", include_raw=False\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              from langchain_ollama import ChatOllama\n",
      " |      \n",
      " |              oai_schema = {\n",
      " |                  'name': 'AnswerWithJustification',\n",
      " |                  'description': 'An answer to the user question along with justification for the answer.',\n",
      " |                  'parameters': {\n",
      " |                      'type': 'object',\n",
      " |                      'properties': {\n",
      " |                          'answer': {'type': 'string'},\n",
      " |                          'justification': {'description': 'A justification for the answer.', 'type': 'string'}\n",
      " |                      },\n",
      " |                     'required': ['answer']\n",
      " |                 }\n",
      " |             }\n",
      " |      \n",
      " |              llm = ChatOllama(model=\"llama3.1\", temperature=0)\n",
      " |              structured_llm = llm.with_structured_output(oai_schema)\n",
      " |      \n",
      " |              structured_llm.invoke(\n",
      " |                  \"What weighs more a pound of bricks or a pound of feathers\"\n",
      " |              )\n",
      " |              # -> {\n",
      " |              #     'answer': 'They weigh the same',\n",
      " |              #     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'\n",
      " |              # }\n",
      " |      \n",
      " |      .. dropdown:: Example: schema=Pydantic class, method=\"json_mode\", include_raw=True\n",
      " |      \n",
      " |          .. code-block::\n",
      " |      \n",
      " |              from langchain_ollama import ChatOllama\n",
      " |              from pydantic import BaseModel\n",
      " |      \n",
      " |              class AnswerWithJustification(BaseModel):\n",
      " |                  answer: str\n",
      " |                  justification: str\n",
      " |      \n",
      " |              llm = ChatOllama(model=\"llama3.1\", temperature=0)\n",
      " |              structured_llm = llm.with_structured_output(\n",
      " |                  AnswerWithJustification,\n",
      " |                  method=\"json_mode\",\n",
      " |                  include_raw=True\n",
      " |              )\n",
      " |      \n",
      " |              structured_llm.invoke(\n",
      " |                  \"Answer the following question. \"\n",
      " |                  \"Make sure to return a JSON blob with keys 'answer' and 'justification'.\\n\\n\"\n",
      " |                  \"What's heavier a pound of bricks or a pound of feathers?\"\n",
      " |              )\n",
      " |              # -> {\n",
      " |              #     'raw': AIMessage(content='{\\n    \"answer\": \"They are both the same weight.\",\\n    \"justification\": \"Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.\" \\n}'),\n",
      " |              #     'parsed': AnswerWithJustification(answer='They are both the same weight.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.'),\n",
      " |              #     'parsing_error': None\n",
      " |              # }\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'_async_client': <class 'ollama._client.AsyncClient...\n",
      " |  \n",
      " |  __class_vars__ = set()\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  __private_attributes__ = {'_async_client': ModelPrivateAttr(), '_clien...\n",
      " |  \n",
      " |  __pydantic_complete__ = True\n",
      " |  \n",
      " |  __pydantic_computed_fields__ = {}\n",
      " |  \n",
      " |  __pydantic_core_schema__ = {'function': {'function': <function ChatOll...\n",
      " |  \n",
      " |  __pydantic_custom_init__ = True\n",
      " |  \n",
      " |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
      " |  \n",
      " |  __pydantic_fields__ = {'base_url': FieldInfo(annotation=Union[str, Non...\n",
      " |  \n",
      " |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
      " |  \n",
      " |  __pydantic_parent_namespace__ = None\n",
      " |  \n",
      " |  __pydantic_post_init__ = 'model_post_init'\n",
      " |  \n",
      " |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n",
      " |      Model...\n",
      " |  \n",
      " |  __pydantic_validator__ = SchemaValidator(title=\"ChatOllama\", validator...\n",
      " |  \n",
      " |  __signature__ = <Signature (*args: Any, name: Optional[str] = No...one...\n",
      " |  \n",
      " |  model_config = {'arbitrary_types_allowed': True, 'extra': 'ignore', 'p...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
      " |  \n",
      " |  __call__(self, messages: 'list[BaseMessage]', stop: 'Optional[list[str]]' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      " |      .. deprecated:: 0.1.7 Use :meth:`~invoke` instead. It will not be removed until langchain-core==1.0.\n",
      " |  \n",
      " |  async agenerate(self, messages: 'list[list[BaseMessage]]', stop: 'Optional[list[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[list[str]]' = None, metadata: 'Optional[dict[str, Any]]' = None, run_name: 'Optional[str]' = None, run_id: 'Optional[uuid.UUID]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      " |      Asynchronously pass a sequence of prompts to a model and return generations.\n",
      " |      \n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |      \n",
      " |      Use this method when you want to:\n",
      " |          1. take advantage of batched calls,\n",
      " |          2. need more output from the model than just the top generated value,\n",
      " |          3. are building chains that are agnostic to the underlying language model\n",
      " |              type (e.g., pure text completion models vs chat models).\n",
      " |      \n",
      " |      Args:\n",
      " |          messages: List of list of messages.\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          callbacks: Callbacks to pass through. Used for executing additional\n",
      " |              functionality, such as logging or streaming, throughout generation.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An LLMResult, which contains a list of candidate Generations for each input\n",
      " |              prompt and additional model provider-specific output.\n",
      " |  \n",
      " |  async agenerate_prompt(self, prompts: 'list[PromptValue]', stop: 'Optional[list[str]]' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      " |      Asynchronously pass a sequence of prompts and return model generations.\n",
      " |      \n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |      \n",
      " |      Use this method when you want to:\n",
      " |          1. take advantage of batched calls,\n",
      " |          2. need more output from the model than just the top generated value,\n",
      " |          3. are building chains that are agnostic to the underlying language model\n",
      " |              type (e.g., pure text completion models vs chat models).\n",
      " |      \n",
      " |      Args:\n",
      " |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      " |              converted to match the format of any language model (string for pure\n",
      " |              text generation models and BaseMessages for chat models).\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          callbacks: Callbacks to pass through. Used for executing additional\n",
      " |              functionality, such as logging or streaming, throughout generation.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An LLMResult, which contains a list of candidate Generations for each input\n",
      " |              prompt and additional model provider-specific output.\n",
      " |  \n",
      " |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[list[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      " |      Default implementation of ainvoke, calls invoke from a thread.\n",
      " |      \n",
      " |      The default implementation allows usage of async code even if\n",
      " |      the Runnable did not implement a native async version of invoke.\n",
      " |      \n",
      " |      Subclasses should override this method if they can run asynchronously.\n",
      " |  \n",
      " |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      " |      .. deprecated:: 0.1.7 Use :meth:`~ainvoke` instead. It will not be removed until langchain-core==1.0.\n",
      " |  \n",
      " |  async apredict_messages(self, messages: 'list[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      " |      .. deprecated:: 0.1.7 Use :meth:`~ainvoke` instead. It will not be removed until langchain-core==1.0.\n",
      " |  \n",
      " |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[list[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[BaseMessageChunk]'\n",
      " |      Default implementation of astream, which calls ainvoke.\n",
      " |      Subclasses should override this method if they support streaming output.\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The input to the Runnable.\n",
      " |          config: The config to use for the Runnable. Defaults to None.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |      \n",
      " |      Yields:\n",
      " |          The output of the Runnable.\n",
      " |  \n",
      " |  call_as_llm(self, message: 'str', stop: 'Optional[list[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      " |      .. deprecated:: 0.1.7 Use :meth:`~invoke` instead. It will not be removed until langchain-core==1.0.\n",
      " |  \n",
      " |  dict(self, **kwargs: 'Any') -> 'dict'\n",
      " |      Return a dictionary of the LLM.\n",
      " |  \n",
      " |  generate(self, messages: 'list[list[BaseMessage]]', stop: 'Optional[list[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[list[str]]' = None, metadata: 'Optional[dict[str, Any]]' = None, run_name: 'Optional[str]' = None, run_id: 'Optional[uuid.UUID]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      " |      Pass a sequence of prompts to the model and return model generations.\n",
      " |      \n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |      \n",
      " |      Use this method when you want to:\n",
      " |          1. take advantage of batched calls,\n",
      " |          2. need more output from the model than just the top generated value,\n",
      " |          3. are building chains that are agnostic to the underlying language model\n",
      " |              type (e.g., pure text completion models vs chat models).\n",
      " |      \n",
      " |      Args:\n",
      " |          messages: List of list of messages.\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          callbacks: Callbacks to pass through. Used for executing additional\n",
      " |              functionality, such as logging or streaming, throughout generation.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An LLMResult, which contains a list of candidate Generations for each input\n",
      " |              prompt and additional model provider-specific output.\n",
      " |  \n",
      " |  generate_prompt(self, prompts: 'list[PromptValue]', stop: 'Optional[list[str]]' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      " |      Pass a sequence of prompts to the model and return model generations.\n",
      " |      \n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |      \n",
      " |      Use this method when you want to:\n",
      " |          1. take advantage of batched calls,\n",
      " |          2. need more output from the model than just the top generated value,\n",
      " |          3. are building chains that are agnostic to the underlying language model\n",
      " |              type (e.g., pure text completion models vs chat models).\n",
      " |      \n",
      " |      Args:\n",
      " |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      " |              converted to match the format of any language model (string for pure\n",
      " |              text generation models and BaseMessages for chat models).\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          callbacks: Callbacks to pass through. Used for executing additional\n",
      " |              functionality, such as logging or streaming, throughout generation.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An LLMResult, which contains a list of candidate Generations for each input\n",
      " |              prompt and additional model provider-specific output.\n",
      " |  \n",
      " |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[list[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      " |      Transform a single input into an output. Override to implement.\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The input to the Runnable.\n",
      " |          config: A config to use when invoking the Runnable.\n",
      " |             The config supports standard keys like 'tags', 'metadata' for tracing\n",
      " |             purposes, 'max_concurrency' for controlling how much work to do\n",
      " |             in parallel, and other keys. Please refer to the RunnableConfig\n",
      " |             for more details.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The output of the Runnable.\n",
      " |  \n",
      " |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      " |      .. deprecated:: 0.1.7 Use :meth:`~invoke` instead. It will not be removed until langchain-core==1.0.\n",
      " |  \n",
      " |  predict_messages(self, messages: 'list[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      " |      .. deprecated:: 0.1.7 Use :meth:`~invoke` instead. It will not be removed until langchain-core==1.0.\n",
      " |  \n",
      " |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[list[str]]' = None, **kwargs: 'Any') -> 'Iterator[BaseMessageChunk]'\n",
      " |      Default implementation of stream, which calls invoke.\n",
      " |      Subclasses should override this method if they support streaming output.\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The input to the Runnable.\n",
      " |          config: The config to use for the Runnable. Defaults to None.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |      \n",
      " |      Yields:\n",
      " |          The output of the Runnable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
      " |  \n",
      " |  raise_deprecation(values: 'dict') -> 'Any'\n",
      " |      Raise deprecation warning if callback_manager is used.\n",
      " |      \n",
      " |      Args:\n",
      " |          values (Dict): Values to validate.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Dict: Validated values.\n",
      " |      \n",
      " |      Raises:\n",
      " |          DeprecationWarning: If callback_manager is used.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
      " |  \n",
      " |  OutputType\n",
      " |      Get the output type for this runnable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
      " |  \n",
      " |  get_num_tokens(self, text: 'str') -> 'int'\n",
      " |      Get the number of tokens present in the text.\n",
      " |      \n",
      " |      Useful for checking if an input fits in a model's context window.\n",
      " |      \n",
      " |      Args:\n",
      " |          text: The string input to tokenize.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The integer number of tokens in the text.\n",
      " |  \n",
      " |  get_num_tokens_from_messages(self, messages: 'list[BaseMessage]', tools: 'Optional[Sequence]' = None) -> 'int'\n",
      " |      Get the number of tokens in the messages.\n",
      " |      \n",
      " |      Useful for checking if an input fits in a model's context window.\n",
      " |      \n",
      " |      **Note**: the base implementation of get_num_tokens_from_messages ignores\n",
      " |      tool schemas.\n",
      " |      \n",
      " |      Args:\n",
      " |          messages: The message inputs to tokenize.\n",
      " |          tools: If provided, sequence of dict, BaseModel, function, or BaseTools\n",
      " |              to be converted to tool schemas.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The sum of the number of tokens across the messages.\n",
      " |  \n",
      " |  get_token_ids(self, text: 'str') -> 'list[int]'\n",
      " |      Return the ordered ids of the tokens in a text.\n",
      " |      \n",
      " |      Args:\n",
      " |          text: The string input to tokenize.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      " |              in the text.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
      " |  \n",
      " |  set_verbose(verbose: 'Optional[bool]') -> 'bool'\n",
      " |      If verbose is None, set it.\n",
      " |      \n",
      " |      This allows users to pass in None as verbose to access the global setting.\n",
      " |      \n",
      " |      Args:\n",
      " |          verbose: The verbosity setting to use.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The verbosity setting to use.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
      " |  \n",
      " |  InputType\n",
      " |      Get the input type for this runnable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.runnables.base.RunnableSerializable:\n",
      " |  \n",
      " |  configurable_alternatives(self, which: 'ConfigurableField', *, default_key: 'str' = 'default', prefix_keys: 'bool' = False, **kwargs: 'Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]]') -> 'RunnableSerializable[Input, Output]'\n",
      " |      Configure alternatives for Runnables that can be set at runtime.\n",
      " |      \n",
      " |      Args:\n",
      " |          which: The ConfigurableField instance that will be used to select the\n",
      " |              alternative.\n",
      " |          default_key: The default key to use if no alternative is selected.\n",
      " |              Defaults to \"default\".\n",
      " |          prefix_keys: Whether to prefix the keys with the ConfigurableField id.\n",
      " |              Defaults to False.\n",
      " |          **kwargs: A dictionary of keys to Runnable instances or callables that\n",
      " |              return Runnable instances.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable with the alternatives configured.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_anthropic import ChatAnthropic\n",
      " |          from langchain_core.runnables.utils import ConfigurableField\n",
      " |          from langchain_openai import ChatOpenAI\n",
      " |      \n",
      " |          model = ChatAnthropic(\n",
      " |              model_name=\"claude-3-sonnet-20240229\"\n",
      " |          ).configurable_alternatives(\n",
      " |              ConfigurableField(id=\"llm\"),\n",
      " |              default_key=\"anthropic\",\n",
      " |              openai=ChatOpenAI()\n",
      " |          )\n",
      " |      \n",
      " |          # uses the default model ChatAnthropic\n",
      " |          print(model.invoke(\"which organization created you?\").content)\n",
      " |      \n",
      " |          # uses ChatOpenAI\n",
      " |          print(\n",
      " |              model.with_config(\n",
      " |                  configurable={\"llm\": \"openai\"}\n",
      " |              ).invoke(\"which organization created you?\").content\n",
      " |          )\n",
      " |  \n",
      " |  configurable_fields(self, **kwargs: 'AnyConfigurableField') -> 'RunnableSerializable[Input, Output]'\n",
      " |      Configure particular Runnable fields at runtime.\n",
      " |      \n",
      " |      Args:\n",
      " |          **kwargs: A dictionary of ConfigurableField instances to configure.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable with the fields configured.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_core.runnables import ConfigurableField\n",
      " |          from langchain_openai import ChatOpenAI\n",
      " |      \n",
      " |          model = ChatOpenAI(max_tokens=20).configurable_fields(\n",
      " |              max_tokens=ConfigurableField(\n",
      " |                  id=\"output_token_number\",\n",
      " |                  name=\"Max tokens in the output\",\n",
      " |                  description=\"The maximum number of tokens in the output\",\n",
      " |              )\n",
      " |          )\n",
      " |      \n",
      " |          # max_tokens = 20\n",
      " |          print(\n",
      " |              \"max_tokens_20: \",\n",
      " |              model.invoke(\"tell me something about chess\").content\n",
      " |          )\n",
      " |      \n",
      " |          # max_tokens = 200\n",
      " |          print(\"max_tokens_200: \", model.with_config(\n",
      " |              configurable={\"output_token_number\": 200}\n",
      " |              ).invoke(\"tell me something about chess\").content\n",
      " |          )\n",
      " |  \n",
      " |  to_json(self) -> 'Union[SerializedConstructor, SerializedNotImplemented]'\n",
      " |      Serialize the Runnable to JSON.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A JSON-serializable representation of the Runnable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from langchain_core.runnables.base.RunnableSerializable:\n",
      " |  \n",
      " |  __orig_bases__ = (<class 'langchain_core.load.serializable.Serializabl...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.load.serializable.Serializable:\n",
      " |  \n",
      " |  __init__(self, *args: Any, **kwargs: Any) -> None\n",
      " |      # Remove default BaseModel init docstring.\n",
      " |  \n",
      " |  __repr_args__(self) -> Any\n",
      " |  \n",
      " |  to_json_not_implemented(self) -> langchain_core.load.serializable.SerializedNotImplemented\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_core.load.serializable.Serializable:\n",
      " |  \n",
      " |  get_lc_namespace() -> list[str]\n",
      " |      Get the namespace of the langchain object.\n",
      " |      \n",
      " |      For example, if the class is `langchain.llms.openai.OpenAI`, then the\n",
      " |      namespace is [\"langchain\", \"llms\", \"openai\"]\n",
      " |  \n",
      " |  is_lc_serializable() -> bool\n",
      " |      Is this class serializable?\n",
      " |      \n",
      " |      By design, even if a class inherits from Serializable, it is not serializable by\n",
      " |      default. This is to prevent accidental serialization of objects that should not\n",
      " |      be serialized.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Whether the class is serializable. Default is False.\n",
      " |  \n",
      " |  lc_id() -> list[str]\n",
      " |      A unique identifier for this class for serialization purposes.\n",
      " |      \n",
      " |      The unique identifier is a list of strings that describes the path\n",
      " |      to the object.\n",
      " |      For example, for the class `langchain.llms.openai.OpenAI`, the id is\n",
      " |      [\"langchain\", \"llms\", \"openai\", \"OpenAI\"].\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.load.serializable.Serializable:\n",
      " |  \n",
      " |  lc_attributes\n",
      " |      List of attribute names that should be included in the serialized kwargs.\n",
      " |      \n",
      " |      These attributes must be accepted by the constructor.\n",
      " |      Default is an empty dictionary.\n",
      " |  \n",
      " |  lc_secrets\n",
      " |      A map of constructor argument names to secret ids.\n",
      " |      \n",
      " |      For example,\n",
      " |          {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from langchain_core.load.serializable.Serializable:\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __copy__(self) -> 'Self'\n",
      " |      Returns a shallow copy of the model.\n",
      " |  \n",
      " |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
      " |      Returns a deep copy of the model.\n",
      " |  \n",
      " |  __delattr__(self, item: 'str') -> 'Any'\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __eq__(self, other: 'Any') -> 'bool'\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __getattr__(self, item: 'str') -> 'Any'\n",
      " |  \n",
      " |  __getstate__(self) -> 'dict[Any, Any]'\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      So `dict(model)` works.\n",
      " |  \n",
      " |  __pretty__(self, fmt: 'typing.Callable[[Any], Any]', **kwargs: 'Any') -> 'typing.Generator[Any, None, None]' from pydantic._internal._repr.Representation\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
      " |  \n",
      " |  __replace__(self, **changes: 'Any') -> 'Self'\n",
      " |      # Because we make use of `@dataclass_transform()`, `__replace__` is already synthesized by\n",
      " |      # type checkers, so we define the implementation in this `if not TYPE_CHECKING:` block:\n",
      " |  \n",
      " |  __repr__(self) -> 'str'\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __repr_name__(self) -> 'str' from pydantic._internal._repr.Representation\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |  \n",
      " |  __repr_recursion__(self, object: 'Any') -> 'str' from pydantic._internal._repr.Representation\n",
      " |      Returns the string representation of a recursive object.\n",
      " |  \n",
      " |  __repr_str__(self, join_str: 'str') -> 'str' from pydantic._internal._repr.Representation\n",
      " |  \n",
      " |  __rich_repr__(self) -> 'RichReprResult' from pydantic._internal._repr.Representation\n",
      " |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
      " |  \n",
      " |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
      " |  \n",
      " |  __str__(self) -> 'str'\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  copy(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      " |      Returns a copy of the model.\n",
      " |      \n",
      " |      !!! warning \"Deprecated\"\n",
      " |          This method is now deprecated; use `model_copy` instead.\n",
      " |      \n",
      " |      If you need `include` or `exclude`, use:\n",
      " |      \n",
      " |      ```python {test=\"skip\" lint=\"skip\"}\n",
      " |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
      " |      data = {**data, **(update or {})}\n",
      " |      copied = self.model_validate(data)\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
      " |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
      " |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
      " |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A copy of the model with included, excluded and updated fields as specified.\n",
      " |  \n",
      " |  json(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -> 'str'\n",
      " |  \n",
      " |  model_copy(self, *, update: 'Mapping[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      " |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#model_copy\n",
      " |      \n",
      " |      Returns a copy of the model.\n",
      " |      \n",
      " |      Args:\n",
      " |          update: Values to change/add in the new model. Note: the data is not validated\n",
      " |              before creating the new model. You should trust this data.\n",
      " |          deep: Set to `True` to make a deep copy of the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          New model instance.\n",
      " |  \n",
      " |  model_dump(self, *, mode: \"Literal['json', 'python'] | str\" = 'python', include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'dict[str, Any]'\n",
      " |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump\n",
      " |      \n",
      " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode: The mode in which `to_python` should run.\n",
      " |              If mode is 'json', the output will only contain JSON serializable types.\n",
      " |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
      " |          include: A set of fields to include in the output.\n",
      " |          exclude: A set of fields to exclude from the output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dictionary representation of the model.\n",
      " |  \n",
      " |  model_dump_json(self, *, indent: 'int | None' = None, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'str'\n",
      " |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump_json\n",
      " |      \n",
      " |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
      " |      \n",
      " |      Args:\n",
      " |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
      " |          include: Field(s) to include in the JSON output.\n",
      " |          exclude: Field(s) to exclude from the JSON output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to serialize using field aliases.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A JSON string representation of the model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef'\n",
      " |      Parameterizes a generic class.\n",
      " |      \n",
      " |      At least, parameterizing a generic class is the *main* thing this method\n",
      " |      does. For example, for some generic class `Foo`, this is called when we\n",
      " |      do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n",
      " |      \n",
      " |      However, note that this method is also called when defining generic\n",
      " |      classes in the first place with `class Foo(Generic[T]): ...`.\n",
      " |  \n",
      " |  __get_pydantic_core_schema__(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -> 'CoreSchema'\n",
      " |      Hook into generating the model's CoreSchema.\n",
      " |      \n",
      " |      Args:\n",
      " |          source: The class we are generating a schema for.\n",
      " |              This will generally be the same as the `cls` argument if this is a classmethod.\n",
      " |          handler: A callable that calls into Pydantic's internal CoreSchema generation logic.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A `pydantic-core` `CoreSchema`.\n",
      " |  \n",
      " |  __get_pydantic_json_schema__(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler', /) -> 'JsonSchemaValue'\n",
      " |      Hook into generating the model's JSON schema.\n",
      " |      \n",
      " |      Args:\n",
      " |          core_schema: A `pydantic-core` CoreSchema.\n",
      " |              You can ignore this argument and call the handler with a new CoreSchema,\n",
      " |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
      " |              or just call the handler with the original schema.\n",
      " |          handler: Call into Pydantic's internal JSON schema generation.\n",
      " |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
      " |              generation fails.\n",
      " |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
      " |              `schema_generator` argument to that function to change JSON schema generation globally\n",
      " |              for a type.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A JSON schema, as a Python object.\n",
      " |  \n",
      " |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None'\n",
      " |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
      " |      only after the class is actually fully initialized. In particular, attributes like `model_fields` will\n",
      " |      be present when this is called.\n",
      " |      \n",
      " |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
      " |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
      " |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
      " |      \n",
      " |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
      " |      any kwargs passed to the class definition that aren't used internally by pydantic.\n",
      " |      \n",
      " |      Args:\n",
      " |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
      " |              by pydantic.\n",
      " |  \n",
      " |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
      " |  \n",
      " |  from_orm(obj: 'Any') -> 'Self'\n",
      " |  \n",
      " |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
      " |      Creates a new instance of the `Model` class with validated data.\n",
      " |      \n",
      " |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |      \n",
      " |      !!! note\n",
      " |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
      " |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
      " |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
      " |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
      " |          an error if extra values are passed, but they will be ignored.\n",
      " |      \n",
      " |      Args:\n",
      " |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n",
      " |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n",
      " |              Otherwise, the field names from the `values` argument will be used.\n",
      " |          values: Trusted or pre-validated data dictionary.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new instance of the `Model` class with validated data.\n",
      " |  \n",
      " |  model_json_schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>, mode: 'JsonSchemaMode' = 'validation') -> 'dict[str, Any]'\n",
      " |      Generates a JSON schema for a model class.\n",
      " |      \n",
      " |      Args:\n",
      " |          by_alias: Whether to use attribute aliases or not.\n",
      " |          ref_template: The reference template.\n",
      " |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
      " |              `GenerateJsonSchema` with your desired modifications\n",
      " |          mode: The mode in which to generate the schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The JSON schema for the given model class.\n",
      " |  \n",
      " |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str'\n",
      " |      Compute the class name for parametrizations of generic classes.\n",
      " |      \n",
      " |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
      " |      \n",
      " |      Args:\n",
      " |          params: Tuple of types of the class. Given a generic class\n",
      " |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
      " |              the value `(str, int)` would be passed to `params`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          String representing the new class where `params` are passed to `cls` as type variables.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
      " |  \n",
      " |  model_rebuild(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'MappingNamespace | None' = None) -> 'bool | None'\n",
      " |      Try to rebuild the pydantic-core schema for the model.\n",
      " |      \n",
      " |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
      " |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
      " |      \n",
      " |      Args:\n",
      " |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
      " |          raise_errors: Whether to raise errors, defaults to `True`.\n",
      " |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
      " |          _types_namespace: The types namespace, defaults to `None`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
      " |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
      " |  \n",
      " |  model_validate(obj: 'Any', *, strict: 'bool | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None) -> 'Self'\n",
      " |      Validate a pydantic model instance.\n",
      " |      \n",
      " |      Args:\n",
      " |          obj: The object to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          from_attributes: Whether to extract data from object attributes.\n",
      " |          context: Additional context to pass to the validator.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValidationError: If the object could not be validated.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The validated model instance.\n",
      " |  \n",
      " |  model_validate_json(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self'\n",
      " |      Usage docs: https://docs.pydantic.dev/2.10/concepts/json/#json-parsing\n",
      " |      \n",
      " |      Validate the given JSON data against the Pydantic model.\n",
      " |      \n",
      " |      Args:\n",
      " |          json_data: The JSON data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n",
      " |  \n",
      " |  model_validate_strings(obj: 'Any', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self'\n",
      " |      Validate the given object with string data against the Pydantic model.\n",
      " |      \n",
      " |      Args:\n",
      " |          obj: The object containing string data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |  \n",
      " |  parse_file(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self'\n",
      " |  \n",
      " |  parse_obj(obj: 'Any') -> 'Self'\n",
      " |  \n",
      " |  parse_raw(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self'\n",
      " |  \n",
      " |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]'\n",
      " |  \n",
      " |  schema_json(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -> 'str'\n",
      " |  \n",
      " |  update_forward_refs(**localns: 'Any') -> 'None'\n",
      " |  \n",
      " |  validate(value: 'Any') -> 'Self'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __fields_set__\n",
      " |  \n",
      " |  model_computed_fields\n",
      " |      Get metadata about the computed fields defined on the model.\n",
      " |      \n",
      " |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n",
      " |      In V3, this property will be removed from the `BaseModel` class.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mapping of computed field names to [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] objects.\n",
      " |  \n",
      " |  model_extra\n",
      " |      Get extra fields set during validation.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
      " |  \n",
      " |  model_fields\n",
      " |      Get metadata about the fields defined on the model.\n",
      " |      \n",
      " |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n",
      " |      In V3, this property will be removed from the `BaseModel` class.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mapping of field names to [`FieldInfo`][pydantic.fields.FieldInfo] objects.\n",
      " |  \n",
      " |  model_fields_set\n",
      " |      Returns the set of fields that have been explicitly set on this model instance.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A set of strings representing the fields that have been set,\n",
      " |              i.e. that were not filled from defaults.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __pydantic_extra__\n",
      " |  \n",
      " |  __pydantic_fields_set__\n",
      " |  \n",
      " |  __pydantic_private__\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __pydantic_root_model__ = False\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.runnables.base.Runnable:\n",
      " |  \n",
      " |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Callable[[Iterator[Any]], Iterator[Other]], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSerializable[Input, Other]'\n",
      " |      Compose this Runnable with another object to create a RunnableSequence.\n",
      " |  \n",
      " |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Other], Any], Callable[[Iterator[Other]], Iterator[Any]], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSerializable[Other, Output]'\n",
      " |      Compose this Runnable with another object to create a RunnableSequence.\n",
      " |  \n",
      " |  async abatch(self, inputs: 'list[Input]', config: 'Optional[Union[RunnableConfig, list[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'list[Output]'\n",
      " |      Default implementation runs ainvoke in parallel using asyncio.gather.\n",
      " |      \n",
      " |      The default implementation of batch works well for IO bound runnables.\n",
      " |      \n",
      " |      Subclasses should override this method if they can batch more efficiently;\n",
      " |      e.g., if the underlying Runnable uses an API which supports a batch mode.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: A list of inputs to the Runnable.\n",
      " |          config: A config to use when invoking the Runnable.\n",
      " |             The config supports standard keys like 'tags', 'metadata' for tracing\n",
      " |             purposes, 'max_concurrency' for controlling how much work to do\n",
      " |             in parallel, and other keys. Please refer to the RunnableConfig\n",
      " |             for more details. Defaults to None.\n",
      " |          return_exceptions: Whether to return exceptions instead of raising them.\n",
      " |              Defaults to False.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of outputs from the Runnable.\n",
      " |  \n",
      " |  async abatch_as_completed(self, inputs: 'Sequence[Input]', config: 'Optional[Union[RunnableConfig, Sequence[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'AsyncIterator[tuple[int, Union[Output, Exception]]]'\n",
      " |      Run ainvoke in parallel on a list of inputs,\n",
      " |      yielding results as they complete.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: A list of inputs to the Runnable.\n",
      " |          config: A config to use when invoking the Runnable.\n",
      " |             The config supports standard keys like 'tags', 'metadata' for tracing\n",
      " |             purposes, 'max_concurrency' for controlling how much work to do\n",
      " |             in parallel, and other keys. Please refer to the RunnableConfig\n",
      " |             for more details. Defaults to None. Defaults to None.\n",
      " |          return_exceptions: Whether to return exceptions instead of raising them.\n",
      " |              Defaults to False.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |      \n",
      " |      Yields:\n",
      " |          A tuple of the index of the input and the output from the Runnable.\n",
      " |  \n",
      " |  as_tool(self, args_schema: 'Optional[type[BaseModel]]' = None, *, name: 'Optional[str]' = None, description: 'Optional[str]' = None, arg_types: 'Optional[dict[str, type]]' = None) -> 'BaseTool'\n",
      " |      .. beta::\n",
      " |         This API is in beta and may change in the future.\n",
      " |      \n",
      " |      Create a BaseTool from a Runnable.\n",
      " |      \n",
      " |      ``as_tool`` will instantiate a BaseTool with a name, description, and\n",
      " |      ``args_schema`` from a Runnable. Where possible, schemas are inferred\n",
      " |      from ``runnable.get_input_schema``. Alternatively (e.g., if the\n",
      " |      Runnable takes a dict as input and the specific dict keys are not typed),\n",
      " |      the schema can be specified directly with ``args_schema``. You can also\n",
      " |      pass ``arg_types`` to just specify the required arguments and their types.\n",
      " |      \n",
      " |      Args:\n",
      " |          args_schema: The schema for the tool. Defaults to None.\n",
      " |          name: The name of the tool. Defaults to None.\n",
      " |          description: The description of the tool. Defaults to None.\n",
      " |          arg_types: A dictionary of argument names to types. Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A BaseTool instance.\n",
      " |      \n",
      " |      Typed dict input:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from typing import List\n",
      " |          from typing_extensions import TypedDict\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |          class Args(TypedDict):\n",
      " |              a: int\n",
      " |              b: List[int]\n",
      " |      \n",
      " |          def f(x: Args) -> str:\n",
      " |              return str(x[\"a\"] * max(x[\"b\"]))\n",
      " |      \n",
      " |          runnable = RunnableLambda(f)\n",
      " |          as_tool = runnable.as_tool()\n",
      " |          as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n",
      " |      \n",
      " |      ``dict`` input, specifying schema via ``args_schema``:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from typing import Any, Dict, List\n",
      " |          from pydantic import BaseModel, Field\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |          def f(x: Dict[str, Any]) -> str:\n",
      " |              return str(x[\"a\"] * max(x[\"b\"]))\n",
      " |      \n",
      " |          class FSchema(BaseModel):\n",
      " |              \"\"\"Apply a function to an integer and list of integers.\"\"\"\n",
      " |      \n",
      " |              a: int = Field(..., description=\"Integer\")\n",
      " |              b: List[int] = Field(..., description=\"List of ints\")\n",
      " |      \n",
      " |          runnable = RunnableLambda(f)\n",
      " |          as_tool = runnable.as_tool(FSchema)\n",
      " |          as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n",
      " |      \n",
      " |      ``dict`` input, specifying schema via ``arg_types``:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from typing import Any, Dict, List\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |          def f(x: Dict[str, Any]) -> str:\n",
      " |              return str(x[\"a\"] * max(x[\"b\"]))\n",
      " |      \n",
      " |          runnable = RunnableLambda(f)\n",
      " |          as_tool = runnable.as_tool(arg_types={\"a\": int, \"b\": List[int]})\n",
      " |          as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n",
      " |      \n",
      " |      String input:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |          def f(x: str) -> str:\n",
      " |              return x + \"a\"\n",
      " |      \n",
      " |          def g(x: str) -> str:\n",
      " |              return x + \"z\"\n",
      " |      \n",
      " |          runnable = RunnableLambda(f) | g\n",
      " |          as_tool = runnable.as_tool()\n",
      " |          as_tool.invoke(\"b\")\n",
      " |      \n",
      " |      .. versionadded:: 0.2.14\n",
      " |  \n",
      " |  assign(self, **kwargs: 'Union[Runnable[dict[str, Any], Any], Callable[[dict[str, Any]], Any], Mapping[str, Union[Runnable[dict[str, Any], Any], Callable[[dict[str, Any]], Any]]]]') -> 'RunnableSerializable[Any, Any]'\n",
      " |      Assigns new fields to the dict output of this Runnable.\n",
      " |      Returns a new Runnable.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_community.llms.fake import FakeStreamingListLLM\n",
      " |          from langchain_core.output_parsers import StrOutputParser\n",
      " |          from langchain_core.prompts import SystemMessagePromptTemplate\n",
      " |          from langchain_core.runnables import Runnable\n",
      " |          from operator import itemgetter\n",
      " |      \n",
      " |          prompt = (\n",
      " |              SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n",
      " |              + \"{question}\"\n",
      " |          )\n",
      " |          llm = FakeStreamingListLLM(responses=[\"foo-lish\"])\n",
      " |      \n",
      " |          chain: Runnable = prompt | llm | {\"str\": StrOutputParser()}\n",
      " |      \n",
      " |          chain_with_assign = chain.assign(hello=itemgetter(\"str\") | llm)\n",
      " |      \n",
      " |          print(chain_with_assign.input_schema.model_json_schema())\n",
      " |          # {'title': 'PromptInput', 'type': 'object', 'properties':\n",
      " |          {'question': {'title': 'Question', 'type': 'string'}}}\n",
      " |          print(chain_with_assign.output_schema.model_json_schema())\n",
      " |          # {'title': 'RunnableSequenceOutput', 'type': 'object', 'properties':\n",
      " |          {'str': {'title': 'Str',\n",
      " |          'type': 'string'}, 'hello': {'title': 'Hello', 'type': 'string'}}}\n",
      " |  \n",
      " |  async astream_events(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, version: \"Literal['v1', 'v2']\", include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[StreamEvent]'\n",
      " |      Generate a stream of events.\n",
      " |      \n",
      " |      Use to create an iterator over StreamEvents that provide real-time information\n",
      " |      about the progress of the Runnable, including StreamEvents from intermediate\n",
      " |      results.\n",
      " |      \n",
      " |      A StreamEvent is a dictionary with the following schema:\n",
      " |      \n",
      " |      - ``event``: **str** - Event names are of the\n",
      " |          format: on_[runnable_type]_(start|stream|end).\n",
      " |      - ``name``: **str** - The name of the Runnable that generated the event.\n",
      " |      - ``run_id``: **str** - randomly generated ID associated with the given execution of\n",
      " |          the Runnable that emitted the event.\n",
      " |          A child Runnable that gets invoked as part of the execution of a\n",
      " |          parent Runnable is assigned its own unique ID.\n",
      " |      - ``parent_ids``: **List[str]** - The IDs of the parent runnables that\n",
      " |          generated the event. The root Runnable will have an empty list.\n",
      " |          The order of the parent IDs is from the root to the immediate parent.\n",
      " |          Only available for v2 version of the API. The v1 version of the API\n",
      " |          will return an empty list.\n",
      " |      - ``tags``: **Optional[List[str]]** - The tags of the Runnable that generated\n",
      " |          the event.\n",
      " |      - ``metadata``: **Optional[Dict[str, Any]]** - The metadata of the Runnable\n",
      " |          that generated the event.\n",
      " |      - ``data``: **Dict[str, Any]**\n",
      " |      \n",
      " |      \n",
      " |      Below is a table that illustrates some events that might be emitted by various\n",
      " |      chains. Metadata fields have been omitted from the table for brevity.\n",
      " |      Chain definitions have been included after the table.\n",
      " |      \n",
      " |      **ATTENTION** This reference table is for the V2 version of the schema.\n",
      " |      \n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | event                | name             | chunk                           | input                                         | output                                          |\n",
      " |      +======================+==================+=================================+===============================================+=================================================+\n",
      " |      | on_chat_model_start  | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chat_model_stream | [model name]     | AIMessageChunk(content=\"hello\") |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chat_model_end    | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} | AIMessageChunk(content=\"hello world\")           |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_llm_start         | [model name]     |                                 | {'input': 'hello'}                            |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_llm_stream        | [model name]     | 'Hello'                         |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_llm_end           | [model name]     |                                 | 'Hello human!'                                |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chain_start       | format_docs      |                                 |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chain_stream      | format_docs      | \"hello world!, goodbye world!\"  |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chain_end         | format_docs      |                                 | [Document(...)]                               | \"hello world!, goodbye world!\"                  |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_tool_start        | some_tool        |                                 | {\"x\": 1, \"y\": \"2\"}                            |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_tool_end          | some_tool        |                                 |                                               | {\"x\": 1, \"y\": \"2\"}                              |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_retriever_start   | [retriever name] |                                 | {\"query\": \"hello\"}                            |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_retriever_end     | [retriever name] |                                 | {\"query\": \"hello\"}                            | [Document(...), ..]                             |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_prompt_start      | [template_name]  |                                 | {\"question\": \"hello\"}                         |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_prompt_end        | [template_name]  |                                 | {\"question\": \"hello\"}                         | ChatPromptValue(messages: [SystemMessage, ...]) |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      \n",
      " |      In addition to the standard events, users can also dispatch custom events (see example below).\n",
      " |      \n",
      " |      Custom events will be only be surfaced with in the `v2` version of the API!\n",
      " |      \n",
      " |      A custom event has following format:\n",
      " |      \n",
      " |      +-----------+------+-----------------------------------------------------------------------------------------------------------+\n",
      " |      | Attribute | Type | Description                                                                                               |\n",
      " |      +===========+======+===========================================================================================================+\n",
      " |      | name      | str  | A user defined name for the event.                                                                        |\n",
      " |      +-----------+------+-----------------------------------------------------------------------------------------------------------+\n",
      " |      | data      | Any  | The data associated with the event. This can be anything, though we suggest making it JSON serializable.  |\n",
      " |      +-----------+------+-----------------------------------------------------------------------------------------------------------+\n",
      " |      \n",
      " |      Here are declarations associated with the standard events shown above:\n",
      " |      \n",
      " |      `format_docs`:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          def format_docs(docs: List[Document]) -> str:\n",
      " |              '''Format the docs.'''\n",
      " |              return \", \".join([doc.page_content for doc in docs])\n",
      " |      \n",
      " |          format_docs = RunnableLambda(format_docs)\n",
      " |      \n",
      " |      `some_tool`:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          @tool\n",
      " |          def some_tool(x: int, y: str) -> dict:\n",
      " |              '''Some_tool.'''\n",
      " |              return {\"x\": x, \"y\": y}\n",
      " |      \n",
      " |      `prompt`:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          template = ChatPromptTemplate.from_messages(\n",
      " |              [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")]\n",
      " |          ).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\n",
      " |      \n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |          async def reverse(s: str) -> str:\n",
      " |              return s[::-1]\n",
      " |      \n",
      " |          chain = RunnableLambda(func=reverse)\n",
      " |      \n",
      " |          events = [\n",
      " |              event async for event in chain.astream_events(\"hello\", version=\"v2\")\n",
      " |          ]\n",
      " |      \n",
      " |          # will produce the following events (run_id, and parent_ids\n",
      " |          # has been omitted for brevity):\n",
      " |          [\n",
      " |              {\n",
      " |                  \"data\": {\"input\": \"hello\"},\n",
      " |                  \"event\": \"on_chain_start\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"chunk\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_stream\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"output\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_end\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |          ]\n",
      " |      \n",
      " |      \n",
      " |      Example: Dispatch Custom Event\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_core.callbacks.manager import (\n",
      " |              adispatch_custom_event,\n",
      " |          )\n",
      " |          from langchain_core.runnables import RunnableLambda, RunnableConfig\n",
      " |          import asyncio\n",
      " |      \n",
      " |      \n",
      " |          async def slow_thing(some_input: str, config: RunnableConfig) -> str:\n",
      " |              \"\"\"Do something that takes a long time.\"\"\"\n",
      " |              await asyncio.sleep(1) # Placeholder for some slow operation\n",
      " |              await adispatch_custom_event(\n",
      " |                  \"progress_event\",\n",
      " |                  {\"message\": \"Finished step 1 of 3\"},\n",
      " |                  config=config # Must be included for python < 3.10\n",
      " |              )\n",
      " |              await asyncio.sleep(1) # Placeholder for some slow operation\n",
      " |              await adispatch_custom_event(\n",
      " |                  \"progress_event\",\n",
      " |                  {\"message\": \"Finished step 2 of 3\"},\n",
      " |                  config=config # Must be included for python < 3.10\n",
      " |              )\n",
      " |              await asyncio.sleep(1) # Placeholder for some slow operation\n",
      " |              return \"Done\"\n",
      " |      \n",
      " |          slow_thing = RunnableLambda(slow_thing)\n",
      " |      \n",
      " |          async for event in slow_thing.astream_events(\"some_input\", version=\"v2\"):\n",
      " |              print(event)\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The input to the Runnable.\n",
      " |          config: The config to use for the Runnable.\n",
      " |          version: The version of the schema to use either `v2` or `v1`.\n",
      " |                   Users should use `v2`.\n",
      " |                   `v1` is for backwards compatibility and will be deprecated\n",
      " |                   in 0.4.0.\n",
      " |                   No default will be assigned until the API is stabilized.\n",
      " |                   custom events will only be surfaced in `v2`.\n",
      " |          include_names: Only include events from runnables with matching names.\n",
      " |          include_types: Only include events from runnables with matching types.\n",
      " |          include_tags: Only include events from runnables with matching tags.\n",
      " |          exclude_names: Exclude events from runnables with matching names.\n",
      " |          exclude_types: Exclude events from runnables with matching types.\n",
      " |          exclude_tags: Exclude events from runnables with matching tags.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |              These will be passed to astream_log as this implementation\n",
      " |              of astream_events is built on top of astream_log.\n",
      " |      \n",
      " |      Yields:\n",
      " |          An async stream of StreamEvents.\n",
      " |      \n",
      " |      Raises:\n",
      " |          NotImplementedError: If the version is not `v1` or `v2`.\n",
      " |  \n",
      " |  async astream_log(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, diff: 'bool' = True, with_streamed_output_list: 'bool' = True, include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]]'\n",
      " |      Stream all output from a Runnable, as reported to the callback system.\n",
      " |      This includes all inner runs of LLMs, Retrievers, Tools, etc.\n",
      " |      \n",
      " |      Output is streamed as Log objects, which include a list of\n",
      " |      Jsonpatch ops that describe how the state of the run has changed in each\n",
      " |      step, and the final state of the run.\n",
      " |      \n",
      " |      The Jsonpatch ops can be applied in order to construct state.\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The input to the Runnable.\n",
      " |          config: The config to use for the Runnable.\n",
      " |          diff: Whether to yield diffs between each step or the current state.\n",
      " |          with_streamed_output_list: Whether to yield the streamed_output list.\n",
      " |          include_names: Only include logs with these names.\n",
      " |          include_types: Only include logs with these types.\n",
      " |          include_tags: Only include logs with these tags.\n",
      " |          exclude_names: Exclude logs with these names.\n",
      " |          exclude_types: Exclude logs with these types.\n",
      " |          exclude_tags: Exclude logs with these tags.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |      \n",
      " |      Yields:\n",
      " |          A RunLogPatch or RunLog object.\n",
      " |  \n",
      " |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      " |      Default implementation of atransform, which buffers input and calls astream.\n",
      " |      Subclasses should override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |      \n",
      " |      Args:\n",
      " |          input: An async iterator of inputs to the Runnable.\n",
      " |          config: The config to use for the Runnable. Defaults to None.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |      \n",
      " |      Yields:\n",
      " |          The output of the Runnable.\n",
      " |  \n",
      " |  batch(self, inputs: 'list[Input]', config: 'Optional[Union[RunnableConfig, list[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'list[Output]'\n",
      " |      Default implementation runs invoke in parallel using a thread pool executor.\n",
      " |      \n",
      " |      The default implementation of batch works well for IO bound runnables.\n",
      " |      \n",
      " |      Subclasses should override this method if they can batch more efficiently;\n",
      " |      e.g., if the underlying Runnable uses an API which supports a batch mode.\n",
      " |  \n",
      " |  batch_as_completed(self, inputs: 'Sequence[Input]', config: 'Optional[Union[RunnableConfig, Sequence[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'Iterator[tuple[int, Union[Output, Exception]]]'\n",
      " |      Run invoke in parallel on a list of inputs,\n",
      " |      yielding results as they complete.\n",
      " |  \n",
      " |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      " |      Bind arguments to a Runnable, returning a new Runnable.\n",
      " |      \n",
      " |      Useful when a Runnable in a chain requires an argument that is not\n",
      " |      in the output of the previous Runnable or included in the user input.\n",
      " |      \n",
      " |      Args:\n",
      " |          kwargs: The arguments to bind to the Runnable.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable with the arguments bound.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_community.chat_models import ChatOllama\n",
      " |          from langchain_core.output_parsers import StrOutputParser\n",
      " |      \n",
      " |          llm = ChatOllama(model='llama2')\n",
      " |      \n",
      " |          # Without bind.\n",
      " |          chain = (\n",
      " |              llm\n",
      " |              | StrOutputParser()\n",
      " |          )\n",
      " |      \n",
      " |          chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
      " |          # Output is 'One two three four five.'\n",
      " |      \n",
      " |          # With bind.\n",
      " |          chain = (\n",
      " |              llm.bind(stop=[\"three\"])\n",
      " |              | StrOutputParser()\n",
      " |          )\n",
      " |      \n",
      " |          chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
      " |          # Output is 'One two'\n",
      " |  \n",
      " |  config_schema(self, *, include: 'Optional[Sequence[str]]' = None) -> 'type[BaseModel]'\n",
      " |      The type of config this Runnable accepts specified as a pydantic model.\n",
      " |      \n",
      " |      To mark a field as configurable, see the `configurable_fields`\n",
      " |      and `configurable_alternatives` methods.\n",
      " |      \n",
      " |      Args:\n",
      " |          include: A list of fields to include in the config schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate config.\n",
      " |  \n",
      " |  get_config_jsonschema(self, *, include: 'Optional[Sequence[str]]' = None) -> 'dict[str, Any]'\n",
      " |      Get a JSON schema that represents the config of the Runnable.\n",
      " |      \n",
      " |      Args:\n",
      " |          include: A list of fields to include in the config schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A JSON schema that represents the config of the Runnable.\n",
      " |      \n",
      " |      .. versionadded:: 0.3.0\n",
      " |  \n",
      " |  get_graph(self, config: 'Optional[RunnableConfig]' = None) -> 'Graph'\n",
      " |      Return a graph representation of this Runnable.\n",
      " |  \n",
      " |  get_input_jsonschema(self, config: 'Optional[RunnableConfig]' = None) -> 'dict[str, Any]'\n",
      " |      Get a JSON schema that represents the input to the Runnable.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A JSON schema that represents the input to the Runnable.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |              def add_one(x: int) -> int:\n",
      " |                  return x + 1\n",
      " |      \n",
      " |              runnable = RunnableLambda(add_one)\n",
      " |      \n",
      " |              print(runnable.get_input_jsonschema())\n",
      " |      \n",
      " |      .. versionadded:: 0.3.0\n",
      " |  \n",
      " |  get_input_schema(self, config: 'Optional[RunnableConfig]' = None) -> 'type[BaseModel]'\n",
      " |      Get a pydantic model that can be used to validate input to the Runnable.\n",
      " |      \n",
      " |      Runnables that leverage the configurable_fields and configurable_alternatives\n",
      " |      methods will have a dynamic input schema that depends on which\n",
      " |      configuration the Runnable is invoked with.\n",
      " |      \n",
      " |      This method allows to get an input schema for a specific configuration.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate input.\n",
      " |  \n",
      " |  get_name(self, suffix: 'Optional[str]' = None, *, name: 'Optional[str]' = None) -> 'str'\n",
      " |      Get the name of the Runnable.\n",
      " |  \n",
      " |  get_output_jsonschema(self, config: 'Optional[RunnableConfig]' = None) -> 'dict[str, Any]'\n",
      " |      Get a JSON schema that represents the output of the Runnable.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A JSON schema that represents the output of the Runnable.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |              def add_one(x: int) -> int:\n",
      " |                  return x + 1\n",
      " |      \n",
      " |              runnable = RunnableLambda(add_one)\n",
      " |      \n",
      " |              print(runnable.get_output_jsonschema())\n",
      " |      \n",
      " |      .. versionadded:: 0.3.0\n",
      " |  \n",
      " |  get_output_schema(self, config: 'Optional[RunnableConfig]' = None) -> 'type[BaseModel]'\n",
      " |      Get a pydantic model that can be used to validate output to the Runnable.\n",
      " |      \n",
      " |      Runnables that leverage the configurable_fields and configurable_alternatives\n",
      " |      methods will have a dynamic output schema that depends on which\n",
      " |      configuration the Runnable is invoked with.\n",
      " |      \n",
      " |      This method allows to get an output schema for a specific configuration.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate output.\n",
      " |  \n",
      " |  get_prompts(self, config: 'Optional[RunnableConfig]' = None) -> 'list[BasePromptTemplate]'\n",
      " |      Return a list of prompts used by this Runnable.\n",
      " |  \n",
      " |  map(self) -> 'Runnable[list[Input], list[Output]]'\n",
      " |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      " |      by calling invoke() with each input.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable that maps a list of inputs to a list of outputs.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |                  from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |                  def _lambda(x: int) -> int:\n",
      " |                      return x + 1\n",
      " |      \n",
      " |                  runnable = RunnableLambda(_lambda)\n",
      " |                  print(runnable.map().invoke([1, 2, 3])) # [2, 3, 4]\n",
      " |  \n",
      " |  pick(self, keys: 'Union[str, list[str]]') -> 'RunnableSerializable[Any, Any]'\n",
      " |      Pick keys from the output dict of this Runnable.\n",
      " |      \n",
      " |      Pick single key:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              import json\n",
      " |      \n",
      " |              from langchain_core.runnables import RunnableLambda, RunnableMap\n",
      " |      \n",
      " |              as_str = RunnableLambda(str)\n",
      " |              as_json = RunnableLambda(json.loads)\n",
      " |              chain = RunnableMap(str=as_str, json=as_json)\n",
      " |      \n",
      " |              chain.invoke(\"[1, 2, 3]\")\n",
      " |              # -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3]}\n",
      " |      \n",
      " |              json_only_chain = chain.pick(\"json\")\n",
      " |              json_only_chain.invoke(\"[1, 2, 3]\")\n",
      " |              # -> [1, 2, 3]\n",
      " |      \n",
      " |      Pick list of keys:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              from typing import Any\n",
      " |      \n",
      " |              import json\n",
      " |      \n",
      " |              from langchain_core.runnables import RunnableLambda, RunnableMap\n",
      " |      \n",
      " |              as_str = RunnableLambda(str)\n",
      " |              as_json = RunnableLambda(json.loads)\n",
      " |              def as_bytes(x: Any) -> bytes:\n",
      " |                  return bytes(x, \"utf-8\")\n",
      " |      \n",
      " |              chain = RunnableMap(\n",
      " |                  str=as_str,\n",
      " |                  json=as_json,\n",
      " |                  bytes=RunnableLambda(as_bytes)\n",
      " |              )\n",
      " |      \n",
      " |              chain.invoke(\"[1, 2, 3]\")\n",
      " |              # -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\n",
      " |      \n",
      " |              json_and_bytes_chain = chain.pick([\"json\", \"bytes\"])\n",
      " |              json_and_bytes_chain.invoke(\"[1, 2, 3]\")\n",
      " |              # -> {\"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\n",
      " |  \n",
      " |  pipe(self, *others: 'Union[Runnable[Any, Other], Callable[[Any], Other]]', name: 'Optional[str]' = None) -> 'RunnableSerializable[Input, Other]'\n",
      " |      Compose this Runnable with Runnable-like objects to make a RunnableSequence.\n",
      " |      \n",
      " |      Equivalent to `RunnableSequence(self, *others)` or `self | others[0] | ...`\n",
      " |      \n",
      " |      Example:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |              def add_one(x: int) -> int:\n",
      " |                  return x + 1\n",
      " |      \n",
      " |              def mul_two(x: int) -> int:\n",
      " |                  return x * 2\n",
      " |      \n",
      " |              runnable_1 = RunnableLambda(add_one)\n",
      " |              runnable_2 = RunnableLambda(mul_two)\n",
      " |              sequence = runnable_1.pipe(runnable_2)\n",
      " |              # Or equivalently:\n",
      " |              # sequence = runnable_1 | runnable_2\n",
      " |              # sequence = RunnableSequence(first=runnable_1, last=runnable_2)\n",
      " |              sequence.invoke(1)\n",
      " |              await sequence.ainvoke(1)\n",
      " |              # -> 4\n",
      " |      \n",
      " |              sequence.batch([1, 2, 3])\n",
      " |              await sequence.abatch([1, 2, 3])\n",
      " |              # -> [4, 6, 8]\n",
      " |  \n",
      " |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      " |      Default implementation of transform, which buffers input and calls astream.\n",
      " |      \n",
      " |      Subclasses should override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |      \n",
      " |      Args:\n",
      " |          input: An iterator of inputs to the Runnable.\n",
      " |          config: The config to use for the Runnable. Defaults to None.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |      \n",
      " |      Yields:\n",
      " |          The output of the Runnable.\n",
      " |  \n",
      " |  with_alisteners(self, *, on_start: 'Optional[AsyncListener]' = None, on_end: 'Optional[AsyncListener]' = None, on_error: 'Optional[AsyncListener]' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind async lifecycle listeners to a Runnable, returning a new Runnable.\n",
      " |      \n",
      " |      on_start: Asynchronously called before the Runnable starts running.\n",
      " |      on_end: Asynchronously called after the Runnable finishes running.\n",
      " |      on_error: Asynchronously called if the Runnable throws an error.\n",
      " |      \n",
      " |      The Run object contains information about the run, including its id,\n",
      " |      type, input, output, error, start_time, end_time, and any tags or metadata\n",
      " |      added to the run.\n",
      " |      \n",
      " |      Args:\n",
      " |          on_start: Asynchronously called before the Runnable starts running.\n",
      " |              Defaults to None.\n",
      " |          on_end: Asynchronously called after the Runnable finishes running.\n",
      " |              Defaults to None.\n",
      " |          on_error: Asynchronously called if the Runnable throws an error.\n",
      " |              Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable with the listeners bound.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |          import time\n",
      " |      \n",
      " |          async def test_runnable(time_to_sleep : int):\n",
      " |              print(f\"Runnable[{time_to_sleep}s]: starts at {format_t(time.time())}\")\n",
      " |              await asyncio.sleep(time_to_sleep)\n",
      " |              print(f\"Runnable[{time_to_sleep}s]: ends at {format_t(time.time())}\")\n",
      " |      \n",
      " |          async def fn_start(run_obj : Runnable):\n",
      " |              print(f\"on start callback starts at {format_t(time.time())}\n",
      " |              await asyncio.sleep(3)\n",
      " |              print(f\"on start callback ends at {format_t(time.time())}\")\n",
      " |      \n",
      " |          async def fn_end(run_obj : Runnable):\n",
      " |              print(f\"on end callback starts at {format_t(time.time())}\n",
      " |              await asyncio.sleep(2)\n",
      " |              print(f\"on end callback ends at {format_t(time.time())}\")\n",
      " |      \n",
      " |          runnable = RunnableLambda(test_runnable).with_alisteners(\n",
      " |              on_start=fn_start,\n",
      " |              on_end=fn_end\n",
      " |          )\n",
      " |          async def concurrent_runs():\n",
      " |              await asyncio.gather(runnable.ainvoke(2), runnable.ainvoke(3))\n",
      " |      \n",
      " |          asyncio.run(concurrent_runs())\n",
      " |          Result:\n",
      " |          on start callback starts at 2024-05-16T14:20:29.637053+00:00\n",
      " |          on start callback starts at 2024-05-16T14:20:29.637150+00:00\n",
      " |          on start callback ends at 2024-05-16T14:20:32.638305+00:00\n",
      " |          on start callback ends at 2024-05-16T14:20:32.638383+00:00\n",
      " |          Runnable[3s]: starts at 2024-05-16T14:20:32.638849+00:00\n",
      " |          Runnable[5s]: starts at 2024-05-16T14:20:32.638999+00:00\n",
      " |          Runnable[3s]: ends at 2024-05-16T14:20:35.640016+00:00\n",
      " |          on end callback starts at 2024-05-16T14:20:35.640534+00:00\n",
      " |          Runnable[5s]: ends at 2024-05-16T14:20:37.640169+00:00\n",
      " |          on end callback starts at 2024-05-16T14:20:37.640574+00:00\n",
      " |          on end callback ends at 2024-05-16T14:20:37.640654+00:00\n",
      " |          on end callback ends at 2024-05-16T14:20:39.641751+00:00\n",
      " |  \n",
      " |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      " |      Bind config to a Runnable, returning a new Runnable.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: The config to bind to the Runnable.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable with the config bound.\n",
      " |  \n",
      " |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'tuple[type[BaseException], ...]' = (<class 'Exception'>,), exception_key: 'Optional[str]' = None) -> 'RunnableWithFallbacksT[Input, Output]'\n",
      " |      Add fallbacks to a Runnable, returning a new Runnable.\n",
      " |      \n",
      " |      The new Runnable will try the original Runnable, and then each fallback\n",
      " |      in order, upon failures.\n",
      " |      \n",
      " |      Args:\n",
      " |          fallbacks: A sequence of runnables to try if the original Runnable fails.\n",
      " |          exceptions_to_handle: A tuple of exception types to handle.\n",
      " |              Defaults to (Exception,).\n",
      " |          exception_key: If string is specified then handled exceptions will be passed\n",
      " |              to fallbacks as part of the input under the specified key. If None,\n",
      " |              exceptions will not be passed to fallbacks. If used, the base Runnable\n",
      " |              and its fallbacks must accept a dictionary as input. Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable that will try the original Runnable, and then each\n",
      " |          fallback in order, upon failures.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              from typing import Iterator\n",
      " |      \n",
      " |              from langchain_core.runnables import RunnableGenerator\n",
      " |      \n",
      " |      \n",
      " |              def _generate_immediate_error(input: Iterator) -> Iterator[str]:\n",
      " |                  raise ValueError()\n",
      " |                  yield \"\"\n",
      " |      \n",
      " |      \n",
      " |              def _generate(input: Iterator) -> Iterator[str]:\n",
      " |                  yield from \"foo bar\"\n",
      " |      \n",
      " |      \n",
      " |              runnable = RunnableGenerator(_generate_immediate_error).with_fallbacks(\n",
      " |                  [RunnableGenerator(_generate)]\n",
      " |                  )\n",
      " |              print(''.join(runnable.stream({}))) #foo bar\n",
      " |      \n",
      " |      Args:\n",
      " |          fallbacks: A sequence of runnables to try if the original Runnable fails.\n",
      " |          exceptions_to_handle: A tuple of exception types to handle.\n",
      " |          exception_key: If string is specified then handled exceptions will be passed\n",
      " |              to fallbacks as part of the input under the specified key. If None,\n",
      " |              exceptions will not be passed to fallbacks. If used, the base Runnable\n",
      " |              and its fallbacks must accept a dictionary as input.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable that will try the original Runnable, and then each\n",
      " |          fallback in order, upon failures.\n",
      " |  \n",
      " |  with_listeners(self, *, on_start: 'Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]' = None, on_end: 'Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]' = None, on_error: 'Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind lifecycle listeners to a Runnable, returning a new Runnable.\n",
      " |      \n",
      " |      on_start: Called before the Runnable starts running, with the Run object.\n",
      " |      on_end: Called after the Runnable finishes running, with the Run object.\n",
      " |      on_error: Called if the Runnable throws an error, with the Run object.\n",
      " |      \n",
      " |      The Run object contains information about the run, including its id,\n",
      " |      type, input, output, error, start_time, end_time, and any tags or metadata\n",
      " |      added to the run.\n",
      " |      \n",
      " |      Args:\n",
      " |          on_start: Called before the Runnable starts running. Defaults to None.\n",
      " |          on_end: Called after the Runnable finishes running. Defaults to None.\n",
      " |          on_error: Called if the Runnable throws an error. Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable with the listeners bound.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |          from langchain_core.tracers.schemas import Run\n",
      " |      \n",
      " |          import time\n",
      " |      \n",
      " |          def test_runnable(time_to_sleep : int):\n",
      " |              time.sleep(time_to_sleep)\n",
      " |      \n",
      " |          def fn_start(run_obj: Run):\n",
      " |              print(\"start_time:\", run_obj.start_time)\n",
      " |      \n",
      " |          def fn_end(run_obj: Run):\n",
      " |              print(\"end_time:\", run_obj.end_time)\n",
      " |      \n",
      " |          chain = RunnableLambda(test_runnable).with_listeners(\n",
      " |              on_start=fn_start,\n",
      " |              on_end=fn_end\n",
      " |          )\n",
      " |          chain.invoke(2)\n",
      " |  \n",
      " |  with_retry(self, *, retry_if_exception_type: 'tuple[type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      " |      Create a new Runnable that retries the original Runnable on exceptions.\n",
      " |      \n",
      " |      Args:\n",
      " |          retry_if_exception_type: A tuple of exception types to retry on.\n",
      " |              Defaults to (Exception,).\n",
      " |          wait_exponential_jitter: Whether to add jitter to the wait\n",
      " |              time between retries. Defaults to True.\n",
      " |          stop_after_attempt: The maximum number of attempts to make before\n",
      " |              giving up. Defaults to 3.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable that retries the original Runnable on exceptions.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |          count = 0\n",
      " |      \n",
      " |      \n",
      " |          def _lambda(x: int) -> None:\n",
      " |              global count\n",
      " |              count = count + 1\n",
      " |              if x == 1:\n",
      " |                  raise ValueError(\"x is 1\")\n",
      " |              else:\n",
      " |                   pass\n",
      " |      \n",
      " |      \n",
      " |          runnable = RunnableLambda(_lambda)\n",
      " |          try:\n",
      " |              runnable.with_retry(\n",
      " |                  stop_after_attempt=2,\n",
      " |                  retry_if_exception_type=(ValueError,),\n",
      " |              ).invoke(1)\n",
      " |          except ValueError:\n",
      " |              pass\n",
      " |      \n",
      " |          assert (count == 2)\n",
      " |      \n",
      " |      \n",
      " |      Args:\n",
      " |          retry_if_exception_type: A tuple of exception types to retry on\n",
      " |          wait_exponential_jitter: Whether to add jitter to the wait time\n",
      " |                                   between retries\n",
      " |          stop_after_attempt: The maximum number of attempts to make before giving up\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable that retries the original Runnable on exceptions.\n",
      " |  \n",
      " |  with_types(self, *, input_type: 'Optional[type[Input]]' = None, output_type: 'Optional[type[Output]]' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind input and output types to a Runnable, returning a new Runnable.\n",
      " |      \n",
      " |      Args:\n",
      " |          input_type: The input type to bind to the Runnable. Defaults to None.\n",
      " |          output_type: The output type to bind to the Runnable. Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable with the types bound.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.runnables.base.Runnable:\n",
      " |  \n",
      " |  config_specs\n",
      " |      List configurable fields for this Runnable.\n",
      " |  \n",
      " |  input_schema\n",
      " |      The type of input this Runnable accepts specified as a pydantic model.\n",
      " |  \n",
      " |  output_schema\n",
      " |      The type of output this Runnable produces specified as a pydantic model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs)\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "help(ChatOllama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no se¨'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alfa=\"**no se¨**\"\n",
    "alfa=alfa.replace('**','')\n",
    "alfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* recording\n",
      "* done recording\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "CHUNK = 1024\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 2\n",
    "RATE = 44100\n",
    "RECORD_SECONDS = 5\n",
    "WAVE_OUTPUT_FILENAME = \"mic2.wav\"\n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "stream = p.open(format=FORMAT,\n",
    "                channels=CHANNELS,\n",
    "                rate=RATE,\n",
    "                input=True,\n",
    "                frames_per_buffer=CHUNK)\n",
    "\n",
    "print(\"* recording\")\n",
    "\n",
    "frames = []\n",
    "\n",
    "for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "    data = stream.read(CHUNK)\n",
    "    frames.append(data)\n",
    "\n",
    "print(\"* done recording\")\n",
    "\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "p.terminate()\n",
    "\n",
    "wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "wf.setnchannels(CHANNELS)\n",
    "wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "wf.setframerate(RATE)\n",
    "wf.writeframes(b''.join(frames))\n",
    "wf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: hola que es la cancion superficial\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "r = sr.Recognizer()\n",
    "\n",
    "hellow=sr.AudioFile('mic2.wav')\n",
    "with hellow as source:\n",
    "    audio = r.record(source)\n",
    "try:\n",
    "    s = r.recognize_google(audio)\n",
    "    print(\"Text: \"+s)\n",
    "except Exception as e:\n",
    "    print(\"Exception: \"+str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method recognize_legacy in module speech_recognition.recognizers.google:\n",
      "\n",
      "recognize_legacy(audio_data: 'AudioData', key: 'str | None' = None, language: 'str' = 'en-US', pfilter: 'ProfanityFilterLevel' = 0, show_all: 'bool' = False, with_confidence: 'bool' = False, *, endpoint: 'str' = 'http://www.google.com/speech-api/v2/recognize') method of speech_recognition.Recognizer instance\n",
      "    Performs speech recognition on ``audio_data`` (an ``AudioData`` instance), using the Google Speech Recognition API.\n",
      "    \n",
      "    The Google Speech Recognition API key is specified by ``key``. If not specified, it uses a generic key that works out of the box. This should generally be used for personal or testing purposes only, as it **may be revoked by Google at any time**.\n",
      "    \n",
      "    To obtain your own API key, simply following the steps on the `API Keys <http://www.chromium.org/developers/how-tos/api-keys>`__ page at the Chromium Developers site. In the Google Developers Console, Google Speech Recognition is listed as \"Speech API\".\n",
      "    \n",
      "    The recognition language is determined by ``language``, an RFC5646 language tag like ``\"en-US\"`` (US English) or ``\"fr-FR\"`` (International French), defaulting to US English. A list of supported language tags can be found in this `StackOverflow answer <http://stackoverflow.com/a/14302134>`__.\n",
      "    \n",
      "    The profanity filter level can be adjusted with ``pfilter``: 0 - No filter, 1 - Only shows the first character and replaces the rest with asterisks. The default is level 0.\n",
      "    \n",
      "    Returns the most likely transcription if ``show_all`` is false (the default). Otherwise, returns the raw API response as a JSON dictionary.\n",
      "    \n",
      "    Raises a ``speech_recognition.UnknownValueError`` exception if the speech is unintelligible. Raises a ``speech_recognition.RequestError`` exception if the speech recognition operation failed, if the key isn't valid, or if there is no internet connection.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(r.recognize_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening...\n",
      "You said: hola como estas\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "recognizer = sr.Recognizer()\n",
    "with sr.Microphone() as source:\n",
    "    print(\"Listening...\")\n",
    "    audio = recognizer.listen(source)\n",
    "\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio)\n",
    "        print(\"You said:\", text)\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Google Speech Recognition could not understand audio\")\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"Could not request results; {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ConversationBufferMemory in module langchain.memory.buffer:\n",
      "\n",
      "class ConversationBufferMemory(langchain.memory.chat_memory.BaseChatMemory)\n",
      " |  ConversationBufferMemory(*args: Any, chat_memory: langchain_core.chat_history.BaseChatMessageHistory = <factory>, output_key: Optional[str] = None, input_key: Optional[str] = None, return_messages: bool = False, human_prefix: str = 'Human', ai_prefix: str = 'AI', memory_key: str = 'history') -> None\n",
      " |  \n",
      " |  .. deprecated:: 0.3.1 Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/ It will not be removed until langchain==1.0.0.\n",
      " |  \n",
      " |  A basic memory implementation that simply stores the conversation history.\n",
      " |  \n",
      " |  This stores the entire conversation history in memory without any\n",
      " |  additional processing.\n",
      " |  \n",
      " |  Note that additional processing may be required in some situations when the\n",
      " |  conversation history is too large to fit in the context window of the model.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ConversationBufferMemory\n",
      " |      langchain.memory.chat_memory.BaseChatMemory\n",
      " |      langchain_core.memory.BaseMemory\n",
      " |      langchain_core.load.serializable.Serializable\n",
      " |      pydantic.main.BaseModel\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *args: Any, **kwargs: Any) -> None from langchain_core.load.serializable.Serializable\n",
      " |  \n",
      " |  async abuffer(self) -> Any\n",
      " |      String buffer of memory.\n",
      " |  \n",
      " |  async abuffer_as_messages(self) -> List[langchain_core.messages.base.BaseMessage]\n",
      " |      Exposes the buffer as a list of messages in case return_messages is False.\n",
      " |  \n",
      " |  async abuffer_as_str(self) -> str\n",
      " |      Exposes the buffer as a string in case return_messages is True.\n",
      " |  \n",
      " |  async aload_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]\n",
      " |      Return key-value pairs given the text input to the chain.\n",
      " |  \n",
      " |  load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]\n",
      " |      Return history buffer.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  buffer\n",
      " |      String buffer of memory.\n",
      " |  \n",
      " |  buffer_as_messages\n",
      " |      Exposes the buffer as a list of messages in case return_messages is False.\n",
      " |  \n",
      " |  buffer_as_str\n",
      " |      Exposes the buffer as a string in case return_messages is True.\n",
      " |  \n",
      " |  memory_variables\n",
      " |      Will always return list of memory variables.\n",
      " |      \n",
      " |      :meta private:\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'ai_prefix': <class 'str'>, 'human_prefix': <class ...\n",
      " |  \n",
      " |  __class_vars__ = set()\n",
      " |  \n",
      " |  __private_attributes__ = {}\n",
      " |  \n",
      " |  __pydantic_complete__ = True\n",
      " |  \n",
      " |  __pydantic_computed_fields__ = {}\n",
      " |  \n",
      " |  __pydantic_core_schema__ = {'cls': <class 'langchain.memory.buffer.Con...\n",
      " |  \n",
      " |  __pydantic_custom_init__ = True\n",
      " |  \n",
      " |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
      " |  \n",
      " |  __pydantic_fields__ = {'ai_prefix': FieldInfo(annotation=str, required...\n",
      " |  \n",
      " |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
      " |  \n",
      " |  __pydantic_parent_namespace__ = None\n",
      " |  \n",
      " |  __pydantic_post_init__ = None\n",
      " |  \n",
      " |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n",
      " |      Model...\n",
      " |  \n",
      " |  __pydantic_validator__ = SchemaValidator(title=\"ConversationBufferMemo...\n",
      " |  \n",
      " |  __signature__ = <Signature (*args: Any, chat_memory: langchain_c...str...\n",
      " |  \n",
      " |  model_config = {'arbitrary_types_allowed': True, 'extra': 'ignore'}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain.memory.chat_memory.BaseChatMemory:\n",
      " |  \n",
      " |  async aclear(self) -> None\n",
      " |      Clear memory contents.\n",
      " |  \n",
      " |  async asave_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None\n",
      " |      Save context from this conversation to buffer.\n",
      " |  \n",
      " |  clear(self) -> None\n",
      " |      Clear memory contents.\n",
      " |  \n",
      " |  save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None\n",
      " |      Save context from this conversation to buffer.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.load.serializable.Serializable:\n",
      " |  \n",
      " |  __repr_args__(self) -> Any\n",
      " |  \n",
      " |  to_json(self) -> Union[langchain_core.load.serializable.SerializedConstructor, langchain_core.load.serializable.SerializedNotImplemented]\n",
      " |      Serialize the object to JSON.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A json serializable object or a SerializedNotImplemented object.\n",
      " |  \n",
      " |  to_json_not_implemented(self) -> langchain_core.load.serializable.SerializedNotImplemented\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_core.load.serializable.Serializable:\n",
      " |  \n",
      " |  get_lc_namespace() -> list[str]\n",
      " |      Get the namespace of the langchain object.\n",
      " |      \n",
      " |      For example, if the class is `langchain.llms.openai.OpenAI`, then the\n",
      " |      namespace is [\"langchain\", \"llms\", \"openai\"]\n",
      " |  \n",
      " |  is_lc_serializable() -> bool\n",
      " |      Is this class serializable?\n",
      " |      \n",
      " |      By design, even if a class inherits from Serializable, it is not serializable by\n",
      " |      default. This is to prevent accidental serialization of objects that should not\n",
      " |      be serialized.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Whether the class is serializable. Default is False.\n",
      " |  \n",
      " |  lc_id() -> list[str]\n",
      " |      A unique identifier for this class for serialization purposes.\n",
      " |      \n",
      " |      The unique identifier is a list of strings that describes the path\n",
      " |      to the object.\n",
      " |      For example, for the class `langchain.llms.openai.OpenAI`, the id is\n",
      " |      [\"langchain\", \"llms\", \"openai\", \"OpenAI\"].\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.load.serializable.Serializable:\n",
      " |  \n",
      " |  lc_attributes\n",
      " |      List of attribute names that should be included in the serialized kwargs.\n",
      " |      \n",
      " |      These attributes must be accepted by the constructor.\n",
      " |      Default is an empty dictionary.\n",
      " |  \n",
      " |  lc_secrets\n",
      " |      A map of constructor argument names to secret ids.\n",
      " |      \n",
      " |      For example,\n",
      " |          {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from langchain_core.load.serializable.Serializable:\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __copy__(self) -> 'Self'\n",
      " |      Returns a shallow copy of the model.\n",
      " |  \n",
      " |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
      " |      Returns a deep copy of the model.\n",
      " |  \n",
      " |  __delattr__(self, item: 'str') -> 'Any'\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __eq__(self, other: 'Any') -> 'bool'\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __getattr__(self, item: 'str') -> 'Any'\n",
      " |  \n",
      " |  __getstate__(self) -> 'dict[Any, Any]'\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      So `dict(model)` works.\n",
      " |  \n",
      " |  __pretty__(self, fmt: 'typing.Callable[[Any], Any]', **kwargs: 'Any') -> 'typing.Generator[Any, None, None]' from pydantic._internal._repr.Representation\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
      " |  \n",
      " |  __replace__(self, **changes: 'Any') -> 'Self'\n",
      " |      # Because we make use of `@dataclass_transform()`, `__replace__` is already synthesized by\n",
      " |      # type checkers, so we define the implementation in this `if not TYPE_CHECKING:` block:\n",
      " |  \n",
      " |  __repr__(self) -> 'str'\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __repr_name__(self) -> 'str' from pydantic._internal._repr.Representation\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |  \n",
      " |  __repr_recursion__(self, object: 'Any') -> 'str' from pydantic._internal._repr.Representation\n",
      " |      Returns the string representation of a recursive object.\n",
      " |  \n",
      " |  __repr_str__(self, join_str: 'str') -> 'str' from pydantic._internal._repr.Representation\n",
      " |  \n",
      " |  __rich_repr__(self) -> 'RichReprResult' from pydantic._internal._repr.Representation\n",
      " |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
      " |  \n",
      " |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
      " |  \n",
      " |  __str__(self) -> 'str'\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  copy(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      " |      Returns a copy of the model.\n",
      " |      \n",
      " |      !!! warning \"Deprecated\"\n",
      " |          This method is now deprecated; use `model_copy` instead.\n",
      " |      \n",
      " |      If you need `include` or `exclude`, use:\n",
      " |      \n",
      " |      ```python {test=\"skip\" lint=\"skip\"}\n",
      " |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
      " |      data = {**data, **(update or {})}\n",
      " |      copied = self.model_validate(data)\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
      " |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
      " |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
      " |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A copy of the model with included, excluded and updated fields as specified.\n",
      " |  \n",
      " |  dict(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False) -> 'Dict[str, Any]'\n",
      " |  \n",
      " |  json(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -> 'str'\n",
      " |  \n",
      " |  model_copy(self, *, update: 'Mapping[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      " |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#model_copy\n",
      " |      \n",
      " |      Returns a copy of the model.\n",
      " |      \n",
      " |      Args:\n",
      " |          update: Values to change/add in the new model. Note: the data is not validated\n",
      " |              before creating the new model. You should trust this data.\n",
      " |          deep: Set to `True` to make a deep copy of the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          New model instance.\n",
      " |  \n",
      " |  model_dump(self, *, mode: \"Literal['json', 'python'] | str\" = 'python', include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'dict[str, Any]'\n",
      " |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump\n",
      " |      \n",
      " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode: The mode in which `to_python` should run.\n",
      " |              If mode is 'json', the output will only contain JSON serializable types.\n",
      " |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
      " |          include: A set of fields to include in the output.\n",
      " |          exclude: A set of fields to exclude from the output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dictionary representation of the model.\n",
      " |  \n",
      " |  model_dump_json(self, *, indent: 'int | None' = None, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'str'\n",
      " |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump_json\n",
      " |      \n",
      " |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
      " |      \n",
      " |      Args:\n",
      " |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
      " |          include: Field(s) to include in the JSON output.\n",
      " |          exclude: Field(s) to exclude from the JSON output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to serialize using field aliases.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A JSON string representation of the model.\n",
      " |  \n",
      " |  model_post_init(self, _BaseModel__context: 'Any') -> 'None'\n",
      " |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n",
      " |      This is useful if you want to do some validation that requires the entire model to be initialized.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef'\n",
      " |  \n",
      " |  __get_pydantic_core_schema__(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -> 'CoreSchema'\n",
      " |      Hook into generating the model's CoreSchema.\n",
      " |      \n",
      " |      Args:\n",
      " |          source: The class we are generating a schema for.\n",
      " |              This will generally be the same as the `cls` argument if this is a classmethod.\n",
      " |          handler: A callable that calls into Pydantic's internal CoreSchema generation logic.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A `pydantic-core` `CoreSchema`.\n",
      " |  \n",
      " |  __get_pydantic_json_schema__(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler', /) -> 'JsonSchemaValue'\n",
      " |      Hook into generating the model's JSON schema.\n",
      " |      \n",
      " |      Args:\n",
      " |          core_schema: A `pydantic-core` CoreSchema.\n",
      " |              You can ignore this argument and call the handler with a new CoreSchema,\n",
      " |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
      " |              or just call the handler with the original schema.\n",
      " |          handler: Call into Pydantic's internal JSON schema generation.\n",
      " |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
      " |              generation fails.\n",
      " |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
      " |              `schema_generator` argument to that function to change JSON schema generation globally\n",
      " |              for a type.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A JSON schema, as a Python object.\n",
      " |  \n",
      " |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None'\n",
      " |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
      " |      only after the class is actually fully initialized. In particular, attributes like `model_fields` will\n",
      " |      be present when this is called.\n",
      " |      \n",
      " |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
      " |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
      " |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
      " |      \n",
      " |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
      " |      any kwargs passed to the class definition that aren't used internally by pydantic.\n",
      " |      \n",
      " |      Args:\n",
      " |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
      " |              by pydantic.\n",
      " |  \n",
      " |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
      " |  \n",
      " |  from_orm(obj: 'Any') -> 'Self'\n",
      " |  \n",
      " |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
      " |      Creates a new instance of the `Model` class with validated data.\n",
      " |      \n",
      " |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |      \n",
      " |      !!! note\n",
      " |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
      " |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
      " |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
      " |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
      " |          an error if extra values are passed, but they will be ignored.\n",
      " |      \n",
      " |      Args:\n",
      " |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n",
      " |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n",
      " |              Otherwise, the field names from the `values` argument will be used.\n",
      " |          values: Trusted or pre-validated data dictionary.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new instance of the `Model` class with validated data.\n",
      " |  \n",
      " |  model_json_schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>, mode: 'JsonSchemaMode' = 'validation') -> 'dict[str, Any]'\n",
      " |      Generates a JSON schema for a model class.\n",
      " |      \n",
      " |      Args:\n",
      " |          by_alias: Whether to use attribute aliases or not.\n",
      " |          ref_template: The reference template.\n",
      " |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
      " |              `GenerateJsonSchema` with your desired modifications\n",
      " |          mode: The mode in which to generate the schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The JSON schema for the given model class.\n",
      " |  \n",
      " |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str'\n",
      " |      Compute the class name for parametrizations of generic classes.\n",
      " |      \n",
      " |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
      " |      \n",
      " |      Args:\n",
      " |          params: Tuple of types of the class. Given a generic class\n",
      " |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
      " |              the value `(str, int)` would be passed to `params`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          String representing the new class where `params` are passed to `cls` as type variables.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
      " |  \n",
      " |  model_rebuild(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'MappingNamespace | None' = None) -> 'bool | None'\n",
      " |      Try to rebuild the pydantic-core schema for the model.\n",
      " |      \n",
      " |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
      " |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
      " |      \n",
      " |      Args:\n",
      " |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
      " |          raise_errors: Whether to raise errors, defaults to `True`.\n",
      " |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
      " |          _types_namespace: The types namespace, defaults to `None`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
      " |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
      " |  \n",
      " |  model_validate(obj: 'Any', *, strict: 'bool | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None) -> 'Self'\n",
      " |      Validate a pydantic model instance.\n",
      " |      \n",
      " |      Args:\n",
      " |          obj: The object to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          from_attributes: Whether to extract data from object attributes.\n",
      " |          context: Additional context to pass to the validator.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValidationError: If the object could not be validated.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The validated model instance.\n",
      " |  \n",
      " |  model_validate_json(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self'\n",
      " |      Usage docs: https://docs.pydantic.dev/2.10/concepts/json/#json-parsing\n",
      " |      \n",
      " |      Validate the given JSON data against the Pydantic model.\n",
      " |      \n",
      " |      Args:\n",
      " |          json_data: The JSON data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n",
      " |  \n",
      " |  model_validate_strings(obj: 'Any', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self'\n",
      " |      Validate the given object with string data against the Pydantic model.\n",
      " |      \n",
      " |      Args:\n",
      " |          obj: The object containing string data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |  \n",
      " |  parse_file(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self'\n",
      " |  \n",
      " |  parse_obj(obj: 'Any') -> 'Self'\n",
      " |  \n",
      " |  parse_raw(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self'\n",
      " |  \n",
      " |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]'\n",
      " |  \n",
      " |  schema_json(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -> 'str'\n",
      " |  \n",
      " |  update_forward_refs(**localns: 'Any') -> 'None'\n",
      " |  \n",
      " |  validate(value: 'Any') -> 'Self'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __fields_set__\n",
      " |  \n",
      " |  model_computed_fields\n",
      " |      Get metadata about the computed fields defined on the model.\n",
      " |      \n",
      " |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n",
      " |      In V3, this property will be removed from the `BaseModel` class.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mapping of computed field names to [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] objects.\n",
      " |  \n",
      " |  model_extra\n",
      " |      Get extra fields set during validation.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
      " |  \n",
      " |  model_fields\n",
      " |      Get metadata about the fields defined on the model.\n",
      " |      \n",
      " |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n",
      " |      In V3, this property will be removed from the `BaseModel` class.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mapping of field names to [`FieldInfo`][pydantic.fields.FieldInfo] objects.\n",
      " |  \n",
      " |  model_fields_set\n",
      " |      Returns the set of fields that have been explicitly set on this model instance.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A set of strings representing the fields that have been set,\n",
      " |              i.e. that were not filled from defaults.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __pydantic_extra__\n",
      " |  \n",
      " |  __pydantic_fields_set__\n",
      " |  \n",
      " |  __pydantic_private__\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __pydantic_root_model__ = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "help(ConversationBufferMemory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

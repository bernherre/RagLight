{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "import torch\n",
    "from langchain.embeddings.huggingface import HuggingFaceBgeEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration variables\n",
    "CHUNK_SIZE = 300\n",
    "CHUNK_OVERLAP = 50\n",
    "\n",
    "# Step 1 : Load the CSV file\n",
    "csv_loader = CSVLoader(file_path='3_building_simple_rag_pipeline\\tesla_motors_data.csv')\n",
    "documents = csv_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2 : Split the text into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3:  Create embeddings\n",
    "embedding_model = HuggingFaceBgeEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = embedding_model.embed_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4:  Store embeddings in FAISS\n",
    "\n",
    "def create_vectorstore(texts, embeddings):\n",
    "    vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "    return vectorstore\n",
    "vector_store =create_vectorstore(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retrieval system\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the LLM\n",
    "llama_model = LlamaForCausalLM.from_pretrained('path/to/llama/model')\n",
    "llama_tokenizer = LlamaTokenizer.from_pretrained('path/to/llama/tokenizer')\n",
    "llm = HuggingFacePipeline(model=llama_model, tokenizer=llama_tokenizer, device=0 if torch.cuda.is_available() else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chatbot function\n",
    "def chatbot(query: str) -> str:\n",
    "    # Retrieve relevant documents\n",
    "    relevant_docs = retriever.retrieve(query)\n",
    "    \n",
    "    # Format the prompt\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"query\", \"documents\"],\n",
    "        template=\"\"\"\n",
    "        You are an expert in information retrieval and natural language processing.\n",
    "        Given the following query and a list of relevant documents, generate a comprehensive response.\n",
    "\n",
    "        Query: {query}\n",
    "\n",
    "        Documents:\n",
    "        {documents}\n",
    "\n",
    "        Response:\n",
    "        \"\"\"\n",
    "    )\n",
    "    formatted_prompt = prompt_template.format(query=query, documents=\"\\n\".join([doc.page_content for doc in relevant_docs]))\n",
    "    \n",
    "    # Generate response using Llama\n",
    "    response = llm(formatted_prompt)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        user_query = input(\"Enter your question (or 'quit' to exit): \").strip()\n",
    "        if user_query.lower() == 'quit':\n",
    "            break\n",
    "        response = chatbot(user_query)\n",
    "        print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
